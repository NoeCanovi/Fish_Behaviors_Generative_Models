{"cells":[{"cell_type":"markdown","metadata":{"id":"jZEHN41HUeAu"},"source":["# Mount and Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8EUaQGJN16tc"},"outputs":[],"source":["#drive.flush_and_unmount()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ur5ATzHC19Or"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gak3Rp6cmIuD"},"outputs":[],"source":["pip install wandb -qq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGhA90r2sBp0"},"outputs":[],"source":["pip install imbalanced-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yEANsBe7jUHZ"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.tensorboard as tb\n","from torch.nn import functional as F\n","\n","\n","import os\n","import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","\n","from tqdm.notebook import tqdm\n","import wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GhUH8QM0M_jG"},"outputs":[],"source":["# Set the device to use for training\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"7l9CygcJVWkn"},"source":["# Autoencoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I4CkELkmWJyN"},"outputs":[],"source":["'''   Dataset   '''\n","\n","'''     imports     '''\n","import os\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd\n","import tqdm\n","import torch\n","\n","'''   custom dataset with trajectories    '''\n","class CustomDataset(Dataset):\n","    def __init__(self, dataset_path):\n","\n","        self.data = []\n","        self.trajectories = []\n","        self.labels = []\n","\n","        self.labels_codec = LabelEncoder()\n","\n","        self._init_dataset(dataset_path)\n","\n","\n","\n","    def __len__(self):\n","        return len(self.trajectories)\n","\n","\n","    def __getitem__(self, idx):\n","        class_id, trajectory = self.data[idx]\n","\n","        trajectory = torch.Tensor(trajectory)\n","\n","        # return both trajectory and class id\n","        return trajectory, class_id\n","\n","    def _init_dataset(self, dataset_path):\n","\n","        list_behaviors = ['C', 'EP', 'FD', 'FM', 'GC', 'JS', 'MA', 'MR', 'NB','NFM', 'S', 'SSP']\n","        #list_behaviors = [item for item in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, item))]\n","        self.labels_codec.fit(list_behaviors)\n","\n","        for root, dirs, filenames in os.walk(dataset_path, topdown=False):\n","            for filename in filenames:\n","                behavior = filename[:filename.find(\"_\")]\n","                behavior_int = self.label_to_int(behavior)\n","                self.labels.append(behavior_int)\n","\n","                filename_path = root + \"/\" + filename\n","                df = pd.read_csv(filename_path)\n","\n","                trajectory = []\n","                for i in range(len(df)):\n","                    traj_point = df.loc[i].tolist()\n","                    trajectory.append(traj_point)\n","\n","                self.trajectories.append(trajectory)\n","\n","                data = [behavior_int, trajectory]\n","                self.data.append(data)\n","\n","    def label_to_int(self, label):\n","        value = self.labels_codec.transform([label])\n","        return value[0]\n","\n","    def int_to_label(self, value):\n","        label = self.labels_codec.inverse_transform([value])\n","        return label[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"51zt6L0yezSM"},"outputs":[],"source":["'''   AutoEncoder Model   '''\n","\n","# imports\n","from typing import List\n","import torch.nn as nn\n","import torch\n","\n","\n","# residual block for encoder\n","class ResConvBlock(nn.Module):\n","    def __init__(\n","        self,\n","        in_channels: int,\n","        out_channels: int,\n","        bias: bool = True\n","    ) -> None:\n","        super(ResConvBlock, self).__init__()\n","\n","        # 1d pooling with kernel = 2, it returns indices\n","        self.pool = nn.MaxPool1d(2, return_indices=True)\n","        self.relu = nn.ReLU(inplace=False)\n","\n","        # encoder block\n","        self.block = nn.Sequential(\n","            nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=1, bias=bias, padding = 1),\n","            nn.ReLU(inplace=False),\n","        )\n","\n","        # convolutional layer shared by the residual block and the skip connection\n","        self.conv1 = self.block[0]\n","\n","    def forward(self, x):\n","        identity = self.conv1(x)\n","        x = self.block(x)\n","        x = x + identity\n","        x = self.relu(x)\n","        return self.pool(x)\n","\n","\n","# residual block for decoder\n","class ResTConvBlock(nn.Module):\n","    def __init__(\n","        self,\n","        in_channels: int,\n","        out_channels: int,\n","        bias: bool = True\n","    ) -> None:\n","        super(ResTConvBlock, self).__init__()\n","        self.relu = nn.ReLU(inplace=False)\n","\n","        # batch normalization\n","        self.bn = nn.BatchNorm1d(out_channels)\n","\n","        # decoder block\n","        self.block = nn.Sequential(\n","            nn.ConvTranspose1d(in_channels, out_channels, kernel_size=3, stride=1, bias=bias, padding=1),\n","            nn.ReLU(inplace=False),\n","        )\n","\n","        # transpose layer shared by the skip and the decoder block\n","        self.devonc3 = self.block[0]\n","\n","    def forward(self, x):\n","        identity = self.devonc3(x)\n","        x = self.block(x)\n","        x = x + identity\n","        x = self.relu(x)\n","        return self.bn(x)\n","\n","\n","class AutoEncoder(nn.Module):\n","    def __init__(\n","        self,\n","        layers: List[int] =  [64, 128],\n","        latent_space: int = 2048,\n","        bias: bool = True\n","    ) -> None:\n","        super(AutoEncoder, self).__init__()\n","        self.layers = layers\n","\n","        # definition encoder blocks\n","        self.enc_block1 = ResConvBlock(3, layers[0])\n","        self.enc_block2 = ResConvBlock(layers[0], layers[1])\n","\n","        # definition linear layer for the latent space\n","        self.encoder_fc = nn.Linear(layers[1] * 3, latent_space, bias=bias)\n","\n","        # definition decoder blocks\n","        self.dec_block2 = ResTConvBlock(layers[1], layers[0])\n","        self.dec_block1 = ResTConvBlock(layers[0], 3)\n","\n","        # definition linear layer for the latent space\n","        self.decoder_fc = nn.Linear(latent_space, layers[1] * 3, bias=bias)\n","\n","        # definition batch norm for normalizing the latent space\n","        self.dec_bn = nn.BatchNorm1d(layers[1])\n","\n","        self.initialize_weights()\n","\n","\n","    # initialize the weights\n","    def initialize_weights(self):\n","        for m in self.modules():\n","          if isinstance(m, nn.Conv1d) or isinstance(m, nn.ConvTranspose1d):\n","                nn.init.kaiming_normal_(m.weight,\n","                                        mode='fan_out',\n","                                        nonlinearity='relu')\n","\n","          elif isinstance(m, nn.BatchNorm1d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","\n","    # encoder function\n","    def encoder(self, x: torch.Tensor) -> torch.Tensor:\n","        x, self.i1 = self.enc_block1(x)\n","        x, self.i2 = self.enc_block2(x)\n","        x = self.encoder_fc(torch.flatten(x, 1))\n","        return x\n","\n","    def freeze_encoder(self):\n","        self.enc_block1.requires_grad = False\n","        self.enc_block2.requires_grad = False\n","        self.encoder_fc.requires_grad = False\n","        print(\"Encoder is frozen\")\n","\n","    # decoder function\n","    def decoder(self, x: torch.Tensor) -> torch.Tensor:\n","        x = self.decoder_fc(x)\n","        x = self.dec_bn(x.view([-1, self.layers[1], 3]))\n","\n","        self.unpool2 = torch.nn.MaxUnpool1d(2)\n","        x = self.unpool2(x, self.i2)\n","        x = self.dec_block2(x)\n","\n","        self.unpool1 = torch.nn.MaxUnpool1d(2)\n","        x = self.unpool1(x, self.i1, output_size = [13])\n","        x = self.dec_block1(x)\n","        return x\n","\n","\n","    # forward function\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        latent_code = self.encoder(x)\n","        rec = self.decoder(latent_code)\n","        return rec, latent_code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MwFbxVQVGyEG"},"outputs":[],"source":["'''   utils   '''\n","\n","class Metric(object):\n","    def __init__(self, name):\n","        self.name = name\n","        self.sum = torch.tensor(0.)\n","        self.n = torch.tensor(0.)\n","\n","    def update(self, val):\n","        self.sum += val.detach().cpu()\n","        self.n += 1\n","\n","    @property\n","    def avg(self):\n","        return self.sum / self.n\n","\n","def random_seed(seed):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","def log(s, nl=True):\n","    print(s, end='\\n' if nl else '')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Etb1Tq69Kpk4"},"outputs":[],"source":["from tqdm import tqdm\n","from torch.nn.functional import mse_loss, l1_loss\n","\n","'''   training    '''\n","def train_auto(autoencoder, dataloader, epoch, reconstruction_loss, optimizer, scheduler, batch_size):\n","\n","    autoencoder.train()\n","\n","    # initialize metrics\n","    trainMSE = Metric('trainMSE')\n","\n","    # start batch training\n","    progress_bar = tqdm(dataloader, desc='description')\n","\n","    for data in progress_bar:\n","\n","            traj = data[0]\n","            labels = data[1]\n","            traj = traj.to(device)\n","\n","            x = torch.transpose(traj, 2,1)\n","\n","            reconstructed_traj, latent_codes = autoencoder(x)\n","            mse_loss = reconstruction_loss(reconstructed_traj, x)\n","            batch_loss = mse_loss\n","\n","            # update metrics\n","            trainMSE.update(mse_loss)\n","\n","\n","            optimizer.zero_grad()\n","            batch_loss.div_(math.ceil(float(len(traj)) / batch_size))\n","            batch_loss.backward()\n","            optimizer.step()\n","\n","\n","    train_log = {'Train MSE': trainMSE.avg.item()}\n","\n","\n","    return train_log"]},{"cell_type":"markdown","metadata":{"id":"9j-UD7Q8CBZQ"},"source":["## Training Autoencoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-RIBl3MkMCMX"},"outputs":[],"source":["'''   imports   '''\n","import torch.nn as nn\n","from torch import optim\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","from tqdm import tqdm\n","import wandb\n","from sklearn.neighbors import KNeighborsClassifier\n","import numpy\n","import warnings\n","from sklearn.manifold import TSNE\n","from numpy import reshape\n","import seaborn as sns\n","\n","'''   main    '''\n","def start(lr=10e-4, batch_size=64, n_worders=2, epochs=10, layers=[64,128],latent_space=2048, bias=True, dataset_path=\"\", wandb_update=False, seed=3):\n","\n","    # ignore warnings\n","    warnings.filterwarnings(\"ignore\")\n","\n","    print(\"initialization\")\n","\n","\n","    # initialize model with dataparallel (according to the # of GPUs available)\n","    autoencoder = torch.nn.DataParallel(AutoEncoder(\n","        layers = [64, 128],\n","        latent_space = 512,\n","        bias = True\n","        ).cuda(),\n","        device_ids=range(torch.cuda.device_count()))\n","\n","\n","    # initialize random seeds\n","    random_seed(seed)\n","\n","    # reconstruction loss, optimizer and scheduler\n","    reconstruction_loss = nn.MSELoss()\n","    optimizer = optim.Adam(autoencoder.parameters(), lr = lr)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n","                                                           mode='min',\n","                                                           factor=0.1,\n","                                                           patience=8,\n","                                                           cooldown=0,\n","                                                           verbose=True)\n","    device = \"cuda:0\"\n","\n","    if wandb_update:\n","      # save model inputs and hyperparameters\n","      config = {\n","        \"lr\": lr,\n","        \"batch_size\": batch_size,\n","        \"epochs\": epochs,\n","        \"layers\": layers,\n","        \"latent_space\": latent_space,\n","      }\n","\n","      # start a new wandb run\n","      run = wandb.init(project='project_name', entity='entity_name', config=config)\n","\n","      # log gradients and model parameters\n","      wandb.watch(autoencoder)\n","\n","    print(\"dataset\")\n","\n","    # load dataset\n","    dataset = CustomDataset(dataset_path)\n","    print(len(dataset))\n","\n","    # dataloader\n","    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","\n","    # training\n","    print('start training')\n","\n","    for epoch in range(epochs):\n","        print('-----------------')\n","\n","        print('train')\n","        # train\n","        train_log = train_auto(autoencoder, loader, epoch, reconstruction_loss, optimizer, scheduler, batch_size)\n","\n","\n","        log('--------------------')\n","        log('Epoch: {}'.format(epoch))\n","        log('Train MSE:{}'.format(train_log['Train MSE']))\n","\n","        '''\n","        if train_log['Train MSE'] < 0.00058:\n","          path = \"/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/Checkpoint/\"\n","          name = \"autoencoderlin\" + str(100. * train_log['Train MSE'])\n","          torch.save({\n","              'epoch': epoch+1,\n","              'model_state_dict': autoencoder.state_dict(),\n","              'optimizer_state_dict': optimizer.state_dict()\n","          }, f'{path}/{name}.pth')\n","\n","        '''\n","        if wandb_update:\n","          wandb.log({'Epochs': epoch + 1,\n","                    'Train Loss': train_log['Train MSE']})\n","\n","\n","        scheduler.step(train_log['Train MSE'])\n","    if wandb_update:\n","      run.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jCSXRtMENTW_"},"outputs":[],"source":["dataset_path = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/dataset/SLB'\n","\n","lr = 10e-4\n","batch_size = 64\n","n_workers = 2\n","epochs = 100\n","layers = [64, 128],\n","latent_space = 512,\n","bias = True\n","wandb_update = True\n","seed = 3\n","\n","start(lr=lr, batch_size=batch_size, n_worders=n_workers, epochs=epochs, layers=layers,latent_space=latent_space, bias=bias, dataset_path=dataset_path, wandb_update=wandb_update, seed=seed)"]},{"cell_type":"markdown","metadata":{"id":"UzKBs2dVPFNy"},"source":["## Eval Autoencoder\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sKUbyocWPEVj"},"outputs":[],"source":["'''   eval    '''\n","def eval(autoencoder, dataloader, reconstruction_loss, dataset):\n","\n","    autoencoder.eval()\n","\n","    testMSE = Metric('testMSE')\n","\n","    rec_traj_d = {}\n","\n","    # start batch testing\n","    progress_bar = tqdm(dataloader, desc='description')\n","    for data in progress_bar:\n","\n","      traj = data[0]\n","      labels = data[1]\n","      traj = traj.to(device)\n","\n","      x = torch.transpose(traj, 2,1)\n","\n","      #with torch.no_grad():\n","      reconstructed_traj, latent_codes = autoencoder(x)\n","      mse_loss = reconstruction_loss(reconstructed_traj, x)\n","\n","\n","      # update metrics\n","      testMSE.update(mse_loss)\n","\n","    test_log = {'Test MSE': testMSE.avg.item()}\n","\n","    return test_log\n","    #return test_log, rec_traj_d\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ncyvPSIbQpyC"},"outputs":[],"source":["'''   imports   '''\n","import pandas as pd\n","from tqdm import tqdm\n","\n","# autoencoder checkpoint path\n","path = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/Checkpoint/checkpoint.pth'\n","\n","# initialize autoencoder\n","autoencoder = torch.nn.DataParallel(AutoEncoder(\n","            layers = [64, 128],\n","            latent_space = 512,\n","            bias = True\n","            ).cuda(),\n","            device_ids=range(torch.cuda.device_count()))\n","\n","# load model state to autoencoder\n","checkpoint = torch.load(path)\n","autoencoder.load_state_dict(checkpoint['model_state_dict'])\n","epoch = checkpoint['epoch']\n","\n","# load dataset\n","dataset_path = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/dataset/SLB/'\n","dataset = CustomDataset(dataset_path)\n","\n","loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n","print(len(dataset))\n","\n","# reconstruction loss\n","reconstruction_loss = nn.MSELoss()\n","\n","# eval step\n","test_log = eval(autoencoder, loader, reconstruction_loss, dataset)\n","#test_log, rec_traj_d = eval(autoencoder, loader, reconstruction_loss, dataset)\n","\n","# print mse error\n","log('Test Loss: {}'.format(test_log['Test MSE']))\n","\n","'''\n","# save some reconstructed trajectories\n","for key in rec_traj_d:\n","\n","  # save trajectory to csv file\n","  rec_traj = rec_traj_d.get(key)\n","  rec_traj = rec_traj.cpu()\n","  rec_traj = rec_traj.detach().numpy()\n","\n","  x = rec_traj[0][0]\n","  y = rec_traj[0][1]\n","  z = rec_traj[0][2]\n","\n","  df = pd.DataFrame(list(zip(x, y, z)), columns=['x','y','ratio_wh'])\n","\n","  file_path = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/dataset/' + key + '.csv'\n","  df.to_csv(file_path, index=False)\n","'''"]},{"cell_type":"markdown","metadata":{"id":"VwRm8E6w-wJr"},"source":["# Latent Codes\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G2W_OcQF-2LJ"},"outputs":[],"source":["'''   eval    '''\n","def eval(autoencoder, dataloader, reconstruction_loss, dataset):\n","\n","    autoencoder.eval()\n","\n","    testMSE = Metric('testMSE')\n","\n","    label_latent_list = []\n","\n","    # start batch testing\n","    progress_bar = tqdm(dataloader, desc='description')\n","    for data in progress_bar:\n","\n","      label_latent = []\n","\n","      traj = data[0]\n","      labels = data[1]\n","      traj = traj.to(device)\n","\n","\n","      x = torch.transpose(traj, 2,1)\n","\n","      with torch.no_grad():\n","          reconstructed_traj, latent_codes = autoencoder(x)\n","          mse_loss = reconstruction_loss(reconstructed_traj, x)\n","\n","\n","\n","      label_latent = [labels, latent_codes]\n","\n","      # update metrics\n","      testMSE.update(mse_loss)\n","\n","    test_log = {'Test MSE': testMSE.avg.item()}\n","\n","    return test_log, label_latent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9S1qowp-2Pj"},"outputs":[],"source":["'''   imports   '''\n","import pandas as pd\n","\n","# autoencoder checkpoint path\n","path = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/Checkpoint/checkpoint.pth'\n","\n","# initialize autoencoder\n","autoencoder = torch.nn.DataParallel(AutoEncoder(\n","            layers = [64, 128],\n","            latent_space = 512,\n","            bias = True\n","            ).cuda(),\n","            device_ids=range(torch.cuda.device_count()))\n","\n","# load model state to autoencoder\n","checkpoint = torch.load(path)\n","autoencoder.load_state_dict(checkpoint['model_state_dict'])\n","epoch = checkpoint['epoch']\n","\n","# load dataset\n","dataset_path = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/dataset/Divided/train'\n","dataset = CustomDataset(dataset_path)\n","print(len(dataset))\n","\n","loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False, num_workers=2, pin_memory=True)\n","\n","\n","# reconstruction loss\n","reconstruction_loss = nn.MSELoss()\n","\n","# eval step\n","test_log, label_latent = eval(autoencoder, loader, reconstruction_loss, dataset)\n","\n","# print mse error\n","log('Test Loss: {}'.format(test_log['Test MSE']))\n","\n","# save csv with latent codes\n","labels = label_latent[0]\n","latents = label_latent[1]\n","\n","label_names = []\n","for label in labels:\n","  label_name = str(dataset.int_to_label(int(label)))\n","  label_names.append(label_name)\n","\n","latent_codes = []\n","for latent in latents:\n","  latent = latent.cpu()\n","  latent = latent.detach().numpy()\n","  print(\"latent\", type(latent))\n","  a = str(latent).replace(\"[\", \"\")\n","  a = a.replace(\"]\",\"\")\n","  a = a.replace(\"  \", \" \")\n","  a = a.strip()\n","  latent_codes.append(a)\n","\n","\n","df = pd.DataFrame(list(zip(label_names,latent_codes)), columns=['label','latent_code'])\n","\n","print(df)\n","\n","file_path = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/dataset/' + 'train' + '.csv'\n","df.to_csv(file_path, index=False)"]},{"cell_type":"markdown","metadata":{"id":"qYlAmF4gF1VM"},"source":["# MLP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cdL-XroPGHEH"},"outputs":[],"source":["'''     imports     '''\n","import imblearn\n","from array import array\n","import numpy as np\n","from collections import Counter\n","from imblearn.over_sampling import SMOTE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWu-ymqlw6mB"},"outputs":[],"source":["'''   Dataset   '''\n","\n","'''     imports     '''\n","import os\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd\n","import torch\n","\n","'''   custom dataset with latent codes    '''\n","class CustomDatasetLatent(Dataset):\n","    def __init__(self, dataset_path):\n","\n","        self.data = []\n","        self.latent_codes = []\n","        self.labels = []\n","\n","        self.labels_codec = LabelEncoder()\n","\n","        self._init_dataset(dataset_path)\n","\n","    def __len__(self):\n","        return len(self.latent_codes)\n","\n","\n","    def __getitem__(self, idx):\n","        class_id, latent_code = self.data[idx]\n","\n","        latent_code = torch.Tensor(latent_code)\n","\n","        # return both trajectory and class id\n","        return latent_code, class_id\n","\n","    def _init_dataset(self, dataset_path):\n","\n","        list_behaviors = ['C', 'EP', 'FD', 'FM', 'GC', 'JS', 'MA', 'MR', 'NB','NFM', 'S', 'SSP']\n","        self.labels_codec.fit(list_behaviors)\n","\n","        df = pd.read_csv(dataset_path)\n","        self.labels = [self.label_to_int(i) for i in df['label'].tolist()]\n","\n","        self.latent_codes = df['latent_code'].tolist()\n","\n","        for label, latent in zip(self.labels, self.latent_codes):\n","\n","          latent = list(latent.split(\" \"))\n","          code = []\n","\n","          for l in latent:\n","            if l != '':\n","                if '\\n' in l:\n","                  l = l.replace('\\n','')\n","\n","                code.append(float(l))\n","\n","          data = [label, code]\n","\n","          self.data.append(data)\n","\n","\n","    def label_to_int(self, label):\n","        value = self.labels_codec.transform([label])\n","        return value[0]\n","\n","    def int_to_label(self, value):\n","        label = self.labels_codec.inverse_transform([value])\n","        return label[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJ38-fUNPFXK"},"outputs":[],"source":["'''   MLP model   '''\n","\n","import torch.nn as nn\n","\n","class MLP(nn.Module):\n","    def __init__(\n","            self,\n","            encoder_latent_space,\n","            output_classes\n","    ):\n","        super().__init__()\n","        self.mlp = nn.Sequential(\n","            nn.Linear(encoder_latent_space, 256),\n","            nn.PReLU(),\n","            nn.Linear(256, 128),\n","            nn.PReLU(),\n","            nn.Linear(128, 64),\n","            nn.PReLU(),\n","            nn.BatchNorm1d(64),\n","            nn.Linear(64, output_classes),\n","        )\n","\n","    def forward(self, x):\n","        return self.mlp(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MIi7WjKine5W"},"outputs":[],"source":["'''   Focal Loss with alpha equal to inverse of class frequencies  '''\n","'''https://saturncloud.io/blog/how-to-use-class-weights-with-focal-loss-in-pytorch-for-imbalanced-multiclass-classification/#:~:text=Focal%20loss%20works%20by%20down,performance%20on%20the%20minority%20class.'''\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, alpha=None, gamma=2):\n","        super(FocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, inputs, targets):\n","        min = 1e-8\n","        max = 1 - min\n","        inputs = torch.clamp(inputs, min, max)\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n","\n","        pt = torch.exp(-ce_loss)\n","        targets = targets.cpu()\n","        pt = pt.cpu()\n","        ce_loss = ce_loss.cpu()\n","        loss = (self.alpha[targets] * (1 - pt) ** self.gamma * ce_loss).mean()\n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CDtnfuswhkQY"},"outputs":[],"source":["'''   Focal Loss with fixed alpha  '''\n","'''https://discuss.pytorch.org/t/focal-loss-for-imbalanced-multi-class-classification-in-pytorch/61289/9'''\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, alpha=None, gamma=2):\n","        super(FocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, outputs, targets):\n","        min = 1e-8\n","        max = 1 - min\n","        outputs = torch.clamp(outputs, min, max)\n","\n","        ce_loss = torch.nn.functional.cross_entropy(outputs, targets, reduction='none') # important to add reduction='none' to keep per-batch-item loss\n","        ce_loss = ce_loss.cpu()\n","        pt = torch.exp(-ce_loss)\n","        targets = targets.cpu()\n","        focal_loss = (self.alpha * (1-pt)**self.gamma * ce_loss).mean() # mean over the batch\n","\n","        return focal_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fny-jQ8BxBCo"},"outputs":[],"source":["'''   utils   '''\n","\n","class Metric(object):\n","    def __init__(self, name):\n","        self.name = name\n","        self.sum = torch.tensor(0.)\n","        self.n = torch.tensor(0.)\n","\n","    def update(self, val):\n","        self.sum += val.detach().cpu()\n","        self.n += 1\n","\n","    @property\n","    def avg(self):\n","        return self.sum / self.n\n","\n","def random_seed(seed):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","def log(s, nl=True):\n","    print(s, end='\\n' if nl else '')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NyeFiBCTQVgS"},"outputs":[],"source":["'''   Dataset   '''\n","\n","'''     imports     '''\n","import os\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd\n","import torch\n","\n","'''   custom dataset with latent codes    '''\n","class CustomDatasetSmote(Dataset):\n","    def __init__(self, x, y):\n","\n","        self.data = []\n","        self.latent_codes = []\n","        self.labels = []\n","\n","        self.labels_codec = LabelEncoder()\n","\n","        self._init_dataset(x, y)\n","\n","    def __len__(self):\n","        return len(self.latent_codes)\n","\n","\n","    def __getitem__(self, idx):\n","        class_id, latent_code = self.data[idx]\n","\n","        latent_code = torch.Tensor(latent_code)\n","\n","        # return both trajectory and class id\n","        return latent_code, class_id\n","\n","    def _init_dataset(self, x, y):\n","\n","        list_behaviors = ['C', 'EP', 'FD', 'FM', 'GC', 'JS', 'MA', 'MR', 'NB','NFM', 'S', 'SSP']\n","        self.labels_codec.fit(list_behaviors)\n","\n","        for label, latent in zip(y,x):\n","            data = [label, latent]\n","            self.data.append(data)\n","\n","        self.labels = y\n","        self.latent_codes = x\n","\n","\n","    def label_to_int(self, label):\n","        value = self.labels_codec.transform([label])\n","        return value[0]\n","\n","    def int_to_label(self, value):\n","        label = self.labels_codec.inverse_transform([value])\n","        return label[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2VPOZR25S5Ez"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","from imblearn.metrics import geometric_mean_score\n","\n","\n","'''   training    '''\n","def train(model, optimizer, scheduler, criterion, epochs, device, train_loader, test_loader, save_weights_name, batch_size, save_model=False, wandb_update=False):\n","    model.to(device)\n","\n","\n","    for e in range(epochs):\n","\n","        '''       training    '''\n","        print('training')\n","        model.train()\n","\n","        trainF = Metric('trainF')\n","\n","        progress_bar = tqdm(train_loader, desc='description')\n","\n","        overall_preds = []\n","        overall_targets = []\n","        softmax = torch.nn.Softmax(dim=1)\n","\n","\n","        for code, targets in progress_bar:\n","\n","            code = code.to(device)\n","            targets = targets.to(device)\n","\n","            # forward\n","            outputs = model(code)\n","\n","            # zero grad\n","            optimizer.zero_grad()\n","\n","            # compute loss\n","            loss = criterion(outputs, targets)\n","\n","            loss.div_(math.ceil(float(len(code)) / batch_size))\n","\n","            # softmax tha outputs and take the max index (argmax), that is the final predicted class\n","            preds = torch.argmax(softmax(outputs), dim=1).cpu()\n","\n","            # save results and targets for accuracy metrics\n","            overall_preds += preds.tolist()\n","            overall_targets += targets.cpu().tolist()\n","\n","            # backpropagate the error\n","            loss.backward()\n","\n","            # clip gradient\n","            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # update weights\n","            optimizer.step()\n","\n","            # update loss\n","            trainF.update(loss)\n","\n","\n","        model.eval()\n","        train_accuracy = accuracy_score(overall_targets, overall_preds)\n","        train_f1 = f1_score(overall_targets, overall_preds, average='macro')\n","\n","\n","\n","\n","        # save loss for epoch\n","        train_log = {'Train F': trainF.avg.item(),\n","                     'Train A': train_accuracy,\n","                     'Train F1': train_f1}\n","\n","        log('--------------------')\n","        log('Epoch: {}'.format(e))\n","        log('Train Focal Loss:{}'.format(train_log['Train F']))\n","        log('Train Accuracy:{}'.format(train_accuracy))\n","        log('Train F1 score:{}'.format(train_f1))\n","\n","\n","        '''       test      '''\n","        print(\"test\")\n","        model.eval()\n","        testF = Metric('testF')\n","\n","        with torch.no_grad():\n","          progress_bar = tqdm(test_loader, desc='description')\n","          overall_preds = []\n","          overall_targets = []\n","          softmax = torch.nn.Softmax(dim=1)\n","\n","          for code, targets in progress_bar:\n","              code = code.to(device)\n","              targets = targets.to(device)\n","\n","              # forward\n","              outputs = model(code)\n","\n","              # compute testing loss\n","              loss = criterion(outputs, targets)\n","\n","              # softmax tha outputs and take the max index (argmax), that is the final predicted class\n","              preds = torch.argmax(softmax(outputs), dim=1).cpu()\n","\n","              # save results and targets for accuracy metrics\n","              overall_preds += preds.tolist()\n","              overall_targets += targets.cpu().tolist()\n","\n","              testF.update(loss)\n","\n","          test_accuracy = accuracy_score(overall_targets, overall_preds)\n","          test_f1 = f1_score(overall_targets, overall_preds, average='macro')\n","\n","          geo_w = geometric_mean_score(overall_targets, overall_preds, average='weighted')\n","          geo_m = geometric_mean_score(overall_targets, overall_preds, average='macro')\n","\n","          test_log = {'Test F': testF.avg.item(),\n","                       'Test A': test_accuracy,\n","                      'Test F1': test_f1,\n","                      'Test Geo W': geo_w,\n","                      'Test Geo M': geo_m}\n","\n","          log('Test Focal Loss:{}'.format(test_log['Test F']))\n","          log('Test Accuracy:{}'.format(test_log['Test A']))\n","          log('Test F1:{}'.format(test_log['Test F1']))\n","          log('Test Geo Weighted:{}'.format(test_log['Test Geo W']))\n","          log('Test Geo Macro:{}'.format(test_log['Test Geo M']))\n","\n","\n","        # save model\n","        #if save_model:\n","\n","\n","        if test_f1 >= 0.4:\n","            path = \"/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/Checkpoint/\"\n","            #name = \"MLP\" + str(100. * train_log['Train A'])\n","            name = \"MLPMS\"  + \"_\" + str(100. * test_f1)\n","            torch.save({\n","                'epoch': e+1,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict()\n","            },f'{path}/{name}.pth')\n","\n","\n","\n","        # update wandb\n","        if wandb_update:\n","          wandb.log({'Epochs': e + 1,\n","                    'Train CE Loss': train_log['Train F'],\n","                     'Train Accuracy': train_log['Train A'],\n","                     'Train F1': train_log['Train F1'],\n","                     'Test CE Loss': test_log['Test F'],\n","                     'Test Accuracy': test_log['Test A'],\n","                     'Test F1': test_log['Test F1'],\n","                     'Test Geo Weighted': test_log['Test Geo W'],\n","                     'Test Geo Macro':test_log['Test Geo M']})\n","\n","        scheduler.step(train_log['Train F'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AovG2tVtOXCS"},"outputs":[],"source":["'''   main    '''\n","def main():\n","    lr = 5e-5\n","    batch_size = 32\n","    n_workers = 2\n","    epochs = 250\n","    wandb_update = True\n","    weight_decay = 1e-3\n","    gamma = 2\n","\n","    latent_space_dim = 512\n","\n","    seed = 3\n","\n","    print(\"begin\")\n","    # initialize random seeds\n","    random_seed(seed)\n","\n","    mlp = MLP(\n","        encoder_latent_space = latent_space_dim,\n","        output_classes = 12\n","    )\n","\n","    #df_weight = pd.read_csv('/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/dataset/train_class_weight.csv')\n","    #class_weights = df_weight['weight'].tolist()\n","\n","    #class_weights = torch.FloatTensor(class_weights)\n","\n","\n","    #criterion = FocalLoss(alpha=class_weights, gamma=gamma)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(mlp.parameters(), lr = lr, weight_decay=weight_decay)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n","                                                           mode='min',\n","                                                           factor=0.1,\n","                                                           patience=10,\n","                                                           cooldown=0,\n","                                                           verbose=True)\n","\n","    device = \"cuda:0\"\n","\n","    if wandb_update:\n","      # save model inputs and hyperparameters\n","      config = {\n","        \"lr\": lr,\n","        \"batch_size\": batch_size,\n","        \"epochs\": epochs,\n","        \"latent_space\": latent_space_dim,\n","        \"loss\": 'CE Loss',\n","        \"weight_decay\": weight_decay,\n","        \"gamma\": gamma\n","      }\n","\n","      # start a new wandb run\n","      run = wandb.init(project='project_name', entity='entity_name', config=config)\n","\n","      # log gradients and model parameters\n","      wandb.watch(mlp)\n","\n","    print(\"dataset\")\n","    # load datasets\n","    # train dataset\n","    path_train = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/dataset/train.csv'\n","    dataset_train = CustomDatasetLatent(path_train)\n","    #dataloader_train = DataLoader(dataset_train, shuffle = True, batch_size=batch_size)\n","    #print(len(dataset_train))\n","\n","    # test dataset\n","    path_test = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/dataset/test.csv'\n","    dataset_test = CustomDatasetLatent(path_test)\n","    dataloader_test = DataLoader(dataset_test, shuffle = True, batch_size=batch_size, num_workers=n_workers, pin_memory=True)\n","    print(len(dataset_test))\n","\n","    # smote\n","    to_sample = {1:33, 3: 114, 4: 18, 7: 9, 11: 57}\n","\n","    oversample = SMOTE(k_neighbors=2, sampling_strategy=to_sample)\n","\n","    x = []\n","    y = []\n","    for i in range(len(dataset_train)):\n","      array = dataset_train[i][0].numpy()\n","      x.append(array)\n","      y.append(dataset_train[i][1])\n","\n","    from collections import Counter\n","    y_counter = Counter(y)\n","    print(y_counter)\n","\n","    total_samples = len(y)\n","\n","    # for each class, calculate weight\n","    class_weights = []\n","    for beh, count in y_counter.items():\n","      weight = 1/ (count /total_samples)\n","      class_weights.append(weight)\n","\n","    print(class_weights)\n","\n","\n","    res = np.array(x)\n","    res = res.reshape(13489, -1)\n","    res, y = oversample.fit_resample(res,y)\n","\n","    #list_y  = y.tolist()\n","    from collections import Counter\n","    print(Counter(y))\n","\n","    dataset = CustomDatasetSmote(res, y)\n","    dataloader_train = DataLoader(dataset, shuffle = True, batch_size=batch_size)\n","    print(len(dataset))\n","\n","\n","    # train\n","    train(\n","        model = mlp,\n","        optimizer = optimizer,\n","        scheduler = scheduler,\n","        criterion = criterion,\n","        epochs = epochs,\n","        device = device,\n","        train_loader = dataloader_train,\n","        test_loader = dataloader_test,\n","        save_weights_name = 'mlp.pth',\n","        batch_size = batch_size,\n","        wandb_update = wandb_update\n","    )\n","\n","    if wandb_update:\n","      run.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JJmQtVUQ7QSn"},"outputs":[],"source":["!wandb login"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8U7AI1-CJ7YS"},"outputs":[],"source":["main()"]},{"cell_type":"markdown","metadata":{"id":"U1t4vDDIJxwQ"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7A-th52ijE-v"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","import pandas as pd\n","import matplotlib\n","import matplotlib.colors as mcolors\n","\n","'''   testing   '''\n","def test(model, criterion, device, data_loader, dataset, load_weights = False, file_name = None, wandb_update=False):\n","\n","    if load_weights:\n","        checkpoint = torch.load(file_name)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        print('Weights loaded.')\n","\n","\n","    model.eval()\n","\n","    testCE = Metric('testCE')\n","\n","    with torch.no_grad():\n","        progress_bar = tqdm(data_loader, desc='description')\n","\n","        overall_preds = []\n","        overall_targets = []\n","        softmax = torch.nn.Softmax(dim=1)\n","        for code, targets in progress_bar:\n","            code = code.to(device)\n","            targets = targets.to(device)\n","\n","            # forward\n","            outputs = model(code)\n","\n","            # compute testing loss\n","            loss = criterion(outputs, targets)\n","\n","            # see confidence?\n","            soft = softmax(outputs).cpu()\n","\n","            # softmax tha outputs and take the max index (argmax), that is the final predicted class\n","            preds = torch.argmax(softmax(outputs), dim=1).cpu()\n","\n","            # save results and targets for accuracy metrics\n","            overall_preds += preds.tolist()\n","            overall_targets += targets.cpu().tolist()\n","\n","            testCE.update(loss)\n","\n","        test_accuracy = accuracy_score(overall_targets, overall_preds)\n","        f1_w = f1_score(overall_targets, overall_preds, average='weighted')\n","        f1_m = f1_score(overall_targets, overall_preds, average='micro')\n","        f1_M = f1_score(overall_targets, overall_preds, average='macro')\n","\n","        print(\"predictions:\")\n","        print(overall_preds)\n","        print(\"labels\")\n","        print(overall_targets)\n","\n","        test_log = {'Test CE': testCE.avg.item(),\n","                      'Test A': test_accuracy}\n","\n","\n","        geo_weighted = geometric_mean_score(overall_targets, overall_preds, average='weighted')\n","        geo_micro = geometric_mean_score(overall_targets, overall_preds, average='micro')\n","        geo_macro = geometric_mean_score(overall_targets, overall_preds, average='macro')\n","\n","        print(\"accuracy: \", test_accuracy)\n","        print(\"f1 score: \", f1_w)\n","        print(\"f1 score macro: \", f1_M)\n","        print(\"geo_weighted: \", geo_weighted)\n","        print(\"geo_micro: \", geo_micro)\n","        print(\"geo_macro: \", geo_macro)\n","\n","\n","\n","        log('Test CE:{}'.format(test_log['Test CE']))\n","        log('Test Accuracy:{}'.format(test_log['Test A']))\n","\n","        print(\"confusion matrix\")\n","        classes = ['C', 'EP', 'FD', 'FM', 'GC', 'JS', 'MA', 'MR', 'NB','NFM', 'S', 'SSP']\n","\n","        cf_matrix = confusion_matrix(overall_targets, overall_preds)\n","\n","        df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n","                            columns = [i for i in classes])\n","        print(\"data frame\")\n","        plt.figure(figsize = (12,7))\n","        sn.heatmap(df_cm, annot=True)\n","\n","        plt.savefig('/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/ConfusionMatrix.png')\n","\n","        plt.show()\n","\n","\n","        preds = [dataset.int_to_label(i) for i in overall_preds]\n","        labels = [dataset.int_to_label(i) for i in overall_targets]\n","\n","\n","        # class accuracy\n","        acc = []\n","        n_test = []\n","        for c in classes:\n","          correct = 0\n","          tot = 0\n","\n","          for i in range(len(labels)):\n","              if labels[i] == c:\n","                  if preds[i] == c:\n","                      correct += 1\n","                  tot += 1\n","          try:\n","              acc_class = correct/tot\n","          except:\n","              print(\"class has no sample\")\n","              acc_class = \"Nan\"\n","\n","          acc.append(acc_class)\n","          n_test.append(tot)\n","\n","\n","          print(\"type\", acc_class, type(acc_class))\n","\n","\n","        print(\"accuracy classes\", acc)\n","        n_tot = [174, 13, 14457, 48, 8, 1019, 67, 4, 198, 895, 31, 23]\n","\n","        df = pd.DataFrame(list(zip( acc, n_tot, n_test,)), index=[i for i in classes], columns = [ 'Class accuracy', '# samples (tot)', '# samples (test)' ])\n","\n","        plt.figure(figsize = (12,7))\n","\n","\n","        norm = mcolors.Normalize(-1,1)\n","\n","        colors = [[norm(-1.0), \"whitesmoke\"],\n","            [norm( 1.0), \"whitesmoke\"]]\n","\n","        cmap = mcolors.LinearSegmentedColormap.from_list(\"\", colors)\n","        hm = sn.heatmap(df, annot=True, cmap=cmap, cbar=False, linecolor=\"black\",fmt='g')\n","\n","\n","        plt.yticks(rotation='horizontal')\n","        plt.xticks()\n","\n","        plt.savefig('/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/ClassAccuracy.png')\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6fVsxmjjE-w"},"outputs":[],"source":["def eval():\n","    lr = 5e-5\n","    batch_size = 64\n","    n_workers = 2\n","    wandb_update = True\n","    weight_decay = 1e-3\n","\n","    latent_space_dim = 512\n","\n","    seed = 3\n","\n","    # initialize random seeds\n","    random_seed(seed)\n","\n","    mlp = MLP(\n","        encoder_latent_space = latent_space_dim,\n","        output_classes = 12\n","    )\n","\n","    # extract class weights from csv file\n","    df_weight = pd.read_csv('/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/dataset/train_class_weight.csv')\n","    class_weights = df_weight['weight'].tolist()\n","    class_weights = torch.FloatTensor(class_weights)\n","\n","    #criterion = FocalLoss(alpha=class_weights, gamma=2)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(mlp.parameters(), lr = lr, weight_decay=weight_decay)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n","                                                           mode='min',\n","                                                           factor=0.1,\n","                                                           patience=8,\n","                                                           cooldown=0,\n","                                                           verbose=True)\n","\n","    device = \"cuda:0\"\n","    mlp.to(device)\n","\n","    # load datasets\n","    # train dataset\n","    path_train = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/dataset/train.csv'\n","    dataset_train = CustomDatasetLatent(path_train)\n","    dataloader_train = DataLoader(dataset_train, shuffle = True, batch_size=batch_size)\n","    print(len(dataset_train))\n","\n","    # test dataset\n","    path_test = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/dataset/test.csv'\n","    dataset_test = CustomDatasetLatent(path_test)\n","    dataloader_test = DataLoader(dataset_test, shuffle = True, batch_size=batch_size, num_workers=n_workers, pin_memory=True)\n","    print(len(dataset_test))\n","\n","\n","    # train\n","    test(\n","        model = mlp,\n","        criterion = criterion,\n","        device = device,\n","        data_loader = dataloader_test,\n","        dataset = dataset_test,\n","        file_name = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/Checkpoint/Checkpoint.pth',\n","        load_weights = True,\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FpyOF6vyKzOJ"},"outputs":[],"source":["eval()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["jZEHN41HUeAu","7l9CygcJVWkn"],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
