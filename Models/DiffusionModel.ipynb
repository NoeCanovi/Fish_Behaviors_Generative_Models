{"cells":[{"cell_type":"markdown","metadata":{"id":"hJgOl87-vBgT"},"source":["# Mount and Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705318721395,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"},"user_tz":-60},"id":"KY7-XUbtu_u-"},"outputs":[],"source":["#drive.flush_and_unmount()"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66549,"status":"ok","timestamp":1705318787941,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"},"user_tz":-60},"id":"P3UNx5e0vJVE","outputId":"a77f1d33-2a31-4db1-96bc-510a359d6a37"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9255,"status":"ok","timestamp":1705318797193,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"},"user_tz":-60},"id":"ZK55wLmyvKe9","outputId":"c658be57-8004-48f0-d970-f7177cfa03fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["pip install wandb -qq"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":13415,"status":"ok","timestamp":1705318810605,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"},"user_tz":-60},"id":"Dx3Fz-sEvL5j"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.tensorboard as tb\n","from torch.nn import functional as F\n","\n","\n","import os\n","import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","\n","from tqdm.notebook import tqdm\n","import wandb"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1705318810606,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"},"user_tz":-60},"id":"cD5TYckrvNOs","outputId":"c23a8001-9291-4a43-853c-1f1bd63963f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["# Set the device to use for training\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1705318810606,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"},"user_tz":-60},"id":"AULwrC4cvQGM"},"outputs":[],"source":["#!unzip '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/Dataset/T_All/t.zip' -d '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/Dataset/T_All/'"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h06p5Z7NUtC7","executionInfo":{"status":"ok","timestamp":1705318810606,"user_tz":-60,"elapsed":4,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"}},"outputId":"fd5dc63b-0e6e-4ab3-d217-ad0611880864"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Prov/video_anomaly_diffusion-main\n"]}],"source":["cd '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Prov/video_anomaly_diffusion-main'"]},{"cell_type":"markdown","metadata":{"id":"Putcl4qICLR7"},"source":["# Diffusion"]},{"cell_type":"markdown","metadata":{"id":"mIsQ1F_4vSsD"},"source":["## Dataset and Utils"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34953,"status":"ok","timestamp":1705318845556,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"},"user_tz":-60},"id":"vCDTrbi7Xnkx","outputId":"b74ead32-eb72-4e6f-e705-8b990c94d511"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting jsonmerge\n","  Downloading jsonmerge-1.9.2-py3-none-any.whl (19 kB)\n","Requirement already satisfied: jsonschema>2.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonmerge) (4.19.2)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>2.4.0->jsonmerge) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>2.4.0->jsonmerge) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>2.4.0->jsonmerge) (0.32.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>2.4.0->jsonmerge) (0.16.2)\n","Installing collected packages: jsonmerge\n","Successfully installed jsonmerge-1.9.2\n","Collecting torchdiffeq\n","  Downloading torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\n","Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from torchdiffeq) (2.1.0+cu121)\n","Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from torchdiffeq) (1.11.4)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy>=1.4.0->torchdiffeq) (1.23.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->torchdiffeq) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->torchdiffeq) (1.3.0)\n","Installing collected packages: torchdiffeq\n","Successfully installed torchdiffeq-0.2.3\n","Collecting accelerate\n","  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.26.1\n"]}],"source":["!pip install jsonmerge\n","!pip install torchdiffeq\n","!pip install accelerate"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":436,"status":"ok","timestamp":1705318845987,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"},"user_tz":-60},"id":"I4CkELkmWJyN"},"outputs":[],"source":["'''   Dataset   '''\n","\n","'''     imports     '''\n","import os\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd\n","import tqdm\n","import torch\n","\n","'''   custom dataset with trajectories    '''\n","class CustomDataset(Dataset):\n","    def __init__(self, dataset_path):\n","\n","        self.data = []\n","        self.trajectories = []\n","        self.labels = []\n","\n","        self.labels_codec = LabelEncoder()\n","\n","        self._init_dataset(dataset_path)\n","\n","\n","\n","    def __len__(self):\n","        return len(self.trajectories)\n","\n","\n","    def __getitem__(self, idx):\n","        class_id, trajectory = self.data[idx]\n","\n","        trajectory = torch.Tensor(trajectory)\n","\n","        # return both trajectory and class id\n","        return trajectory, class_id\n","\n","    def _init_dataset(self, dataset_path):\n","\n","        #list_behaviors = [item for item in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, item))]\n","        list_behaviors = ['C', 'EP', 'FD', 'FM', 'GC', 'JS', 'MA', 'MR', 'NB','NFM', 'S', 'SSP']\n","        self.labels_codec.fit(list_behaviors)\n","\n","        for root, dirs, filenames in os.walk(dataset_path, topdown=False):\n","            for filename in filenames:\n","                behavior = filename[:filename.find(\"_\")]\n","                behavior_int = self.label_to_int(behavior)\n","                self.labels.append(behavior_int)\n","\n","                filename_path = root + \"/\" + filename\n","                df = pd.read_csv(filename_path)\n","\n","                trajectory = []\n","                for i in range(len(df)):\n","                    traj_point = df.loc[i].tolist()\n","                    trajectory.append(traj_point)\n","\n","                self.trajectories.append(trajectory)\n","\n","                data = [behavior_int, trajectory]\n","                self.data.append(data)\n","\n","    def label_to_int(self, label):\n","        value = self.labels_codec.transform([label])\n","        return value[0]\n","\n","    def int_to_label(self, value):\n","        label = self.labels_codec.inverse_transform([value])\n","        return label[0]"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":308,"status":"ok","timestamp":1705318846293,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"},"user_tz":-60},"id":"qj346WRMDlax"},"outputs":[],"source":["'''   utils   '''\n","\n","class Metric(object):\n","    def __init__(self, name):\n","        self.name = name\n","        self.sum = torch.tensor(0.)\n","        self.n = torch.tensor(0.)\n","\n","    def update(self, val):\n","        self.sum += val.detach().cpu()\n","        self.n += 1\n","\n","    @property\n","    def avg(self):\n","        return self.sum / self.n\n","\n","def random_seed(seed):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","def log(s, nl=True):\n","    print(s, end='\\n' if nl else '')\n","\n","# loc = mean, scale = std\n","def find_sigma(loc, scale):\n","  sigma_min = np.exp(loc - 5 * scale)\n","  sigma_max = np.exp(loc + 5 * scale)\n","\n","  return sigma_min, sigma_max\n","\n","import random\n","def seed_everything(seed=42):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705318846293,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"},"user_tz":-60},"id":"nCK-VIACMbVK"},"outputs":[],"source":["from contextlib import contextmanager\n","@contextmanager\n","def train_mode(model, mode=True):\n","    \"\"\"A context manager that places a model into training mode and restores\n","    the previous mode on exit.\"\"\"\n","\n","    modes = [module.training for module in model.modules()]\n","\n","    len_modes = len(modes)\n","\n","\n","    try:\n","        yield model.train(mode)\n","    finally:\n","\n","        for i, module in enumerate(model.modules()):\n","            module.training = True\n","            #if i < len_modes:\n","              #module.training = modes[i]"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705318846293,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"},"user_tz":-60},"id":"obu6bNmkMc85"},"outputs":[],"source":["def eval_mode(model):\n","    \"\"\"A context manager that places a model into evaluation mode and restores\n","    the previous mode on exit.\"\"\"\n","    return train_mode(model, False)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"uloilLnsOJEX","executionInfo":{"status":"ok","timestamp":1705318853799,"user_tz":-60,"elapsed":7508,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"}}},"outputs":[],"source":["# imports\n","import k_diffusion as K\n","from collections import Counter\n","import accelerate\n","import torch.nn as nn\n","from torch import optim\n","from torch.utils.data import Dataset, DataLoader\n","from copy import deepcopy"]},{"cell_type":"markdown","metadata":{"id":"cZPw-SItCKdk"},"source":["## Train"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1705318853800,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"},"user_tz":-60},"id":"JEBwddxiXt1U"},"outputs":[],"source":["import k_diffusion as K\n","from collections import Counter\n","import accelerate\n","import torch.nn as nn\n","from torch import optim\n","from torch.utils.data import Dataset, DataLoader\n","from copy import deepcopy\n","\n","def training_diffusion(config_tune, n_epochs = 2, dataset=None, batch_size = 32, seed = 3, grad_accum_steps=1, wandb_update = False):\n","\n","\n","\n","    print(\"config\")\n","\n","    # it loads configurations from k_diffusion/config.py\n","    config_file_name = 'configs/config_ano.json'\n","    config = K.config.load_config(open(config_file_name))\n","\n","    model_config = config['model']\n","    dataset_config = config['dataset']\n","    opt_config = config['optimizer']\n","    sched_config = config['lr_sched']\n","    ema_sched_config = config['ema_sched'] # ema = esponentially moving average\n","\n","    model_config['sigma_max'] = config_tune[\"sigma_max\"]\n","    model_config['sigma_min'] = config_tune[\"sigma_min\"]\n","    model_config['sigma_data'] = config_tune[\"sigma_data\"]\n","    model_config['sigma_sample_density']['mean'] = config_tune[\"ssd_mean\"]\n","    model_config['sigma_sample_density']['std'] = config_tune[\"ssd_std\"]\n","    opt_config['lr'] = config_tune[\"lr\"]\n","    opt_config['weight_decay'] = config_tune[\"weight_decay\"]\n","\n","    print(\"dataset\")\n","\n","\n","\n","    feat = []\n","    labels = []\n","    for i in range(len(dataset)):\n","      array = dataset[i][0].numpy()\n","      feat.append(array)\n","      labels.append(dataset[i][1])\n","\n","\n","    print(\"features\")\n","\n","    print(len(feat))\n","    print(feat[0])\n","    print(feat[0][0])\n","\n","    # feature size\n","    feat_size = 13\n","\n","    print(feat_size)\n","\n","    if model_config['input_size'] != feat_size:\n","      print('input size replace.')\n","      model_config['input_size'] = feat_size\n","\n","\n","    print(\"accelerators\")\n","    # accelerators\n","    ddp_kwargs = accelerate.DistributedDataParallelKwargs(find_unused_parameters=model_config['skip_stages'] > 0)\n","    accelerator = accelerate.Accelerator(kwargs_handlers=[ddp_kwargs],\n","                                         gradient_accumulation_steps=grad_accum_steps)\n","\n","    device = accelerator.device\n","    print(f'Process {accelerator.process_index} using device: {device}', flush=True)\n","\n","    # seeds for accelerator\n","    if seed is not None:\n","        seeds = torch.randint(-2 ** 63, 2 ** 63 - 1, [accelerator.num_processes],\n","                              generator=torch.Generator().manual_seed(seed))\n","        torch.manual_seed(seeds[accelerator.process_index])\n","\n","    print(\"model\")\n","    feat_size = 13\n","\n","    gvad_model = K.models.denoiser_model.GVADModel(\n","        feat_size,\n","    )\n","\n","    # tensorboard\n","    if accelerator.is_main_process:\n","        from torch.utils.tensorboard import SummaryWriter\n","        print('Parameters:', K.utils.n_params(gvad_model))\n","        com_str = ''\n","        com_str += f'_lr:{config_tune[\"lr\"]}'\n","        com_str += f'_s_max:{config_tune[\"sigma_max\"]}'\n","        com_str += f'_s_min:{config_tune[\"sigma_min\"]}'\n","        com_str += f'_s_data:{config_tune[\"sigma_data\"]}'\n","        com_str += f'_ssd_mean:{config_tune[\"ssd_mean\"]}'\n","        com_str += f'_ssd_std:{config_tune[\"ssd_std\"]}'\n","        com_str += f'_weight_decay:{config_tune[\"weight_decay\"]}'\n","        writer = SummaryWriter(comment=com_str)\n","\n","    # optimizer\n","    if opt_config['type'] == 'adamw':\n","        opt = optim.AdamW(gvad_model.parameters(),\n","                          lr=opt_config['lr'],\n","                          betas=tuple(opt_config['betas']),\n","                          eps=opt_config['eps'],\n","                          weight_decay=opt_config['weight_decay'])\n","    elif opt_config['type'] == 'sgd':\n","        opt = optim.SGD(gvad_model.parameters(),\n","                        lr=opt_config['lr'],\n","                        momentum=opt_config.get('momentum', 0.),\n","                        nesterov=opt_config.get('nesterov', False),\n","                        weight_decay=opt_config.get('weight_decay', 0.))\n","    else:\n","        raise ValueError('Invalid optimizer type')\n","\n","    if sched_config['type'] == 'inverse':\n","      sched = K.utils.InverseLR(opt,\n","                              inv_gamma=sched_config['inv_gamma'],\n","                              power=sched_config['power'],\n","                              warmup=sched_config['warmup'])\n","    elif sched_config['type'] == 'exponential':\n","        sched = K.utils.ExponentialLR(opt,\n","                                      num_steps=sched_config['num_steps'],\n","                                      decay=sched_config['decay'],\n","                                      warmup=sched_config['warmup'])\n","    else:\n","      raise ValueError('Invalid schedule type')\n","\n","\n","    # assert = checks if something is true or not\n","    assert ema_sched_config['type'] == 'inverse'\n","    ema_sched = K.utils.EMAWarmup(power=ema_sched_config['power'],\n","                                  max_value=ema_sched_config['max_value'])\n","\n","    if accelerator.is_main_process:\n","        try:\n","            print('Number of items in dataset:', len(dataset))\n","        except TypeError:\n","            pass\n","\n","\n","    # dataloader\n","    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n","\n","    model, opt, train_dl = accelerator.prepare(gvad_model, opt, loader)\n","    inner_model = model.ae\n","\n","    # sample density\n","    sigma_min = model_config['sigma_min']\n","    sigma_max = model_config['sigma_max']\n","    sample_density = K.config.make_sample_density(model_config)\n","\n","    model_denoiser = K.config.make_denoiser_wrapper(config)(inner_model)\n","    model_ema = deepcopy(model_denoiser)\n","\n","\n","    # wandb\n","    if wandb_update:\n","      # save model inputs and hyperparameters\n","      config = {\n","        \"lr\": opt_config['lr'],\n","        \"batch_size\": batch_size,\n","        \"epochs\": n_epochs,\n","        \"config\": config_tune,\n","      }\n","\n","      # start a new wandb run\n","      run = wandb.init(project='project_name', entity='entity_name', config=config)\n","\n","      # log gradients and model parameters\n","      wandb.watch(model)\n","\n","\n","\n","    step = 0\n","\n","\n","    print(\"training\")\n","    # training\n","\n","    for epoch in range(n_epochs):\n","\n","        #trainMSE = Metric('trainMSE')\n","        for batch in tqdm(loader, disable=not accelerator.is_main_process):\n","            with accelerator.accumulate(model):\n","\n","                reals = batch[0]\n","\n","                # resize correctly the trajectory: batch size x 13 x 3\n","                reals = torch.transpose(reals, 2,1)\n","\n","                # torch randn returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1\n","                # noise\n","                # noise sampling\n","                noise = torch.randn_like(reals)\n","\n","                sigma = sample_density([reals.shape[0]], device=device)\n","\n","\n","                reals = reals.cuda()\n","                noise = noise.cuda()\n","                sigma = sigma.cuda()\n","\n","                g_losses = model_denoiser.loss(reals, noise, sigma) # losses with the batch\n","\n","                gen_dist = accelerator.gather(g_losses)\n","\n","                loss = gen_dist.mean()\n","\n","                accelerator.backward(loss)\n","\n","                # update\n","                opt.step()\n","                sched.step()\n","                opt.zero_grad()\n","\n","                #print(\"ema\")\n","\n","                if accelerator.sync_gradients:\n","                    ema_decay = ema_sched.get_value()\n","                    K.utils.ema_update(model_denoiser, model_ema, ema_decay)\n","                    #K.utils.ema_update(model, model_ema, ema_decay)\n","                    ema_sched.step()\n","\n","\n","            if accelerator.is_main_process:\n","                writer.add_scalar('Epoch/train', epoch, step)\n","                writer.add_scalar('Loss/train', loss.item(), step)\n","                writer.add_scalar('ema_decay/train', ema_decay, step)\n","\n","                if step % 25 == 0:\n","                      tqdm.write(f'Epoch: {epoch}, step: {step}, loss: {loss.item():g}')\n","\n","\n","            step += 1\n","\n","        print(\"epoch, step and loss\", epoch, step, loss.item())\n","\n","\n","        @torch.no_grad()\n","        @eval_mode(model_ema)\n","        def evaluate(model, sigma_min, sigma_max, loader, n_start=0):\n","\n","            tqdm.write('Evaluating...')\n","\n","            sigmas = K.sampling.get_sigmas_karras(10, sigma_min, sigma_max, rho=7., device=device)\n","            sigmas = sigmas[n_start:]\n","\n","            sigmas = sigmas.cuda()\n","\n","            sample_noise = torch.randn([1, feat_size], device=device) * sigma_max\n","            sample_noise = sample_noise.cuda()\n","\n","            def sample_fn(x_real, sigmas, model):\n","                #x_real = x_real.to(device)\n","                x_real = x_real.cuda()\n","                x_real = torch.transpose(x_real, 2,1)\n","\n","                x = sample_noise + x_real\n","\n","                x = x.cuda()\n","                sigmas = sigmas.cuda()\n","                model = model.cuda()\n","\n","                x_0 = K.sampling.sample_lms(model_ema, x, sigmas, disable=True)\n","                return x_0\n","\n","            def compute_eval_outs_aot(sample_fn, dl,sigmas,model):\n","                outputs = []\n","                test_loss = []\n","\n","                for batch in tqdm(dl):\n","                    feat = batch[0]\n","\n","\n","                    g_dists = sample_fn(feat,sigmas,model_ema)\n","\n","                    g_dists = accelerator.gather(g_dists)\n","\n","                    feat = torch.transpose(feat, 2,1)\n","                    feat = feat.cuda()\n","                    g_dists = g_dists.cuda()\n","\n","                    loss =  model.loss(feat, g_dists).mean().item()\n","\n","                    test_loss.append(loss)\n","\n","                    outputs.append(g_dists)\n","\n","\n","                g_dists = torch.cat(outputs)\n","\n","\n","                test_loss =  sum(test_loss)/len(test_loss)\n","\n","                return test_loss\n","\n","            test_loss = compute_eval_outs_aot(sample_fn, loader,sigmas,model)\n","\n","            return test_loss\n","\n","\n","        test_loss = evaluate(model=model, sigma_min=model_config['sigma_min'], sigma_max=model_config['sigma_max'], loader=loader, n_start=0)\n","\n","        print(\"Test Loss:\", test_loss)\n","\n","\n","        # update wandb\n","        if wandb_update:\n","          wandb.log({'Epochs': epoch + 1,\n","                    'Step': step,\n","                    'Train Loss': loss.item(),\n","                    'Test Loss': test_loss})\n","\n","        '''\n","        if test_loss < 0.0025:\n","          path = \"/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/Checkpoint/\"\n","          name = \"AATest\" + str(100. * test_loss)\n","          torch.save({\n","              'epoch': epoch+1,\n","              'model_state_dict': model.state_dict(),\n","              'optimizer_state_dict': opt.state_dict(),\n","              'model_ema': model_ema.state_dict(),\n","              'sched': sched.state_dict(),\n","              'ema_sched': ema_sched.state_dict(),\n","          }, f'{path}/{name}.pth')\n","\n","\n","        if loss.item() < 0.1:\n","          path = \"/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/Checkpoint/\"\n","          name = \"Train\" + str(100. * loss.item())\n","          torch.save({\n","              'epoch': epoch+1,\n","              'model_state_dict': model.state_dict(),\n","              'optimizer_state_dict': opt.state_dict(),\n","              'model_ema': model_ema.state_dict(),\n","              'sched': sched.state_dict(),\n","              'ema_sched': ema_sched.state_dict(),\n","          }, f'{path}/{name}.pth')\n","\n","          '''\n","\n","    # end wandb run\n","    if wandb_update:\n","      run.finish()"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1705318853800,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"},"user_tz":-60},"id":"1oU_YHsuCKmd"},"outputs":[],"source":["''' start training  '''\n","\n","from tqdm import tqdm\n","from torch import multiprocessing as mp\n","import warnings\n","import k_diffusion.models.denoiser_model\n","def start(dataset):\n","  warnings.filterwarnings(\"ignore\")\n","  print(\"initialization\")\n","\n","\n","  epochs = 100\n","  batch_size = 512\n","  seed = 3\n","\n","  # initialize random seeds\n","  seed_everything(seed)\n","\n","  # select start method for multiprocess\n","  #mp.set_start_method('spawn')\n","\n","  # A bool that controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs\n","  torch.backends.cuda.matmul.allow_tf32 = True\n","\n","  std = [0.25]\n","  mean = [-3]\n","  for ssd_std in std:\n","    for ssd_mean in mean:\n","        sigma_min, sigma_max = find_sigma(ssd_mean, ssd_std)\n","\n","        config = {\n","            \"sigma_min\": sigma_min,\n","            \"sigma_max\": sigma_max,\n","            \"sigma_data\": 0.2574,\n","            \"ssd_mean\": ssd_mean,\n","            \"ssd_std\": ssd_std,\n","            \"lr\": 0.002,\n","            \"weight_decay\": 0.0005,\n","        }\n","\n","\n","        training_diffusion(config, n_epochs = epochs, dataset = dataset, batch_size = batch_size, seed = seed, wandb_update = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PnFmwnkPii7K"},"outputs":[],"source":["!wandb login"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-gThyZf4pCqX"},"outputs":[],"source":["dataset_path = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/dataset/SLB'\n","# load dataset\n","dataset = CustomDataset(dataset_path)\n","print(len(dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjvAF2jDacA8"},"outputs":[],"source":["start(dataset)"]},{"cell_type":"markdown","metadata":{"id":"6n_iQN1o8bPn"},"source":["## Eval\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ajrNNmIjlS-v"},"outputs":[],"source":["''' Setting same noise. If not evaluation is different every time as the noise is randomly extracted each time '''\n","seed_noise_path = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/Checkpoint/Noise/noise.pth'\n","if os.path.exists(seed_noise_path):\n","      seed_noise = torch.load(seed_noise_path, map_location=device)\n","else:\n","      seed_noise = torch.randn([1, 13], device=device)\n","      torch.save(seed_noise, seed_noise_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NyRG1DPx8cgO"},"outputs":[],"source":["'''   imports   '''\n","import pandas as pd\n","from tqdm import tqdm\n","import k_diffusion.models.auto\n","import k_diffusion as K\n","import k_diffusion.config as con\n","\n","\n","config_file_name = 'configs/config_ano.json'\n","config = K.config.load_config(open(config_file_name))\n","\n","model_config = config['model']\n","dataset_config = config['dataset']\n","opt_config = config['optimizer']\n","sched_config = config['lr_sched']\n","ema_sched_config = config['ema_sched'] # ema = esponentially moving average\n","\n","\n","opt_config['lr'] = 0.002\n","opt_config['weight_decay'] = 0.00005\n","\n","# autoencoder checkpoint path\n","path = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/Checkpoint/checkpoint.pth'\n","\n","# initialize model\n","gvad_model = K.models.denoiser_model.GVADModel(\n","    13,\n",")\n","\n","inner_model = gvad_model.ae\n","\n","model_denoiser = K.config.make_denoiser_wrapper(config)(inner_model)\n","model_ema = deepcopy(model_denoiser)\n","\n","\n","# load model state to denoiser model\n","checkpoint = torch.load(path)\n","gvad_model.load_state_dict(checkpoint['model_state_dict'])\n","epoch = checkpoint['epoch']\n","\n","model_ema.load_state_dict(checkpoint['model_ema'])\n","\n","opt = optim.AdamW(gvad_model.parameters(),\n","                  lr=opt_config['lr'],\n","                  betas=tuple(opt_config['betas']),\n","                  eps=opt_config['eps'],\n","                  weight_decay=opt_config['weight_decay'])\n","\n","# scheduler\n","sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,\n","                                                  mode='min',\n","                                                  factor=0.1,\n","                                                  patience=8,\n","                                                  cooldown=0,\n","                                                  verbose=True)\n","\n","\n","\n","\n","ema_sched = K.utils.EMAWarmup(power=ema_sched_config['power'],\n","                              max_value=ema_sched_config['max_value'])\n","\n","\n","opt.load_state_dict(checkpoint['optimizer_state_dict'])\n","sched.load_state_dict(checkpoint['sched'])\n","ema_sched.load_state_dict(checkpoint['ema_sched'])\n","\n","\n","@eval_mode(model_ema)\n","@torch.no_grad()\n","def evaluate(model, sigma_min, sigma_max, loader, n_start, model_ema):\n","\n","    tqdm.write('Evaluating...')\n","\n","    sigmas = K.sampling.get_sigmas_karras(10, sigma_min, sigma_max, rho=7., device=device)\n","    sigmas = sigmas[n_start:]\n","\n","    sigmas = sigmas.cuda()\n","\n","    # noise is extracted randomly\n","    #sample_noise = torch.randn([1, 13], device=device) * sigma_max\n","\n","    # noise is extracted by file\n","    sample_noise = seed_noise * sigmas[0]\n","\n","    sample_noise = sample_noise.cuda()\n","\n","\n","    def sample_fn(x_real, sigmas, model, model_ema):\n","\n","        x_real = x_real.cuda()\n","        x_real = torch.transpose(x_real, 2,1)\n","\n","        x = sample_noise + x_real\n","\n","        x = x.cuda()\n","        sigmas = sigmas.cuda()\n","        model = model.cuda()\n","        model_ema = model_ema.cuda()\n","\n","        x_0 = K.sampling.sample_lms(model_ema, x, sigmas, disable=True)\n","\n","        return x_0\n","\n","    def compute_eval_outs_aot(sample_fn, dl,sigmas,model):\n","        outputs = []\n","        test_loss = []\n","\n","\n","\n","        for batch in tqdm(dl):\n","\n","            feat = batch[0]\n","\n","\n","            x_0 = sample_fn(feat,sigmas,model, model_ema)\n","\n","            feat = torch.transpose(feat, 2,1)\n","            feat = feat.cuda()\n","            x_0 = x_0.cuda()\n","\n","            loss =  model.loss(feat, x_0).mean().item()\n","            print(\"Loss batch\", loss)\n","\n","            test_loss.append(loss)\n","            outputs.append(x_0)\n","\n","            '''\n","            # save trajectory to csv file\n","            key = str(batch[1])\n","            x_0 = x_0.cpu()\n","            x_0 = x_0.detach().numpy()\n","            x = x_0[0][0]\n","            y = x_0[0][1]\n","            z = x_0[0][2]\n","\n","\n","            df = pd.DataFrame(list(zip(x, y, z)), columns=['x','y','ratio_wh'])\n","\n","            file_path = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/Dataset/Plot_SameButT/'+ 'MR'  + str(n_start) + '.csv'\n","            df.to_csv(file_path, index=False)\n","            '''\n","\n","\n","        g_dists = torch.cat(outputs)\n","        test_loss =  sum(test_loss)/len(test_loss)\n","        print(\"loss total: \", test_loss)\n","\n","        return g_dists\n","\n","\n","    gen_preds = compute_eval_outs_aot(sample_fn, loader,sigmas,model)\n","\n","\n","dataset_path = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/dataset/Plot/MR/'\n","dataset = CustomDataset(dataset_path)\n","\n","loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False, num_workers=0, pin_memory=True)\n","print(len(dataset))\n","\n","ssd_mean = -4\n","ssd_std = 0.5\n","sigma_min, sigma_max = find_sigma(ssd_mean, ssd_std)\n","for i in range(10):\n","  print(\" Evaluating with t = \", i)\n","  evaluate(model=gvad_model, sigma_min=sigma_min, sigma_max=sigma_max, loader=loader, n_start=i, model_ema = model_ema)"]},{"cell_type":"markdown","metadata":{"id":"2PUZGG4wqbdA"},"source":["# Learned Features\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yX_xUtb_vKqi"},"outputs":[],"source":["''' Setting same noise. If not evaluation is different every time as the noise is randomly extracted each time '''\n","seed_noise_path = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/Checkpoint/Noise/noise.pth'\n","if os.path.exists(seed_noise_path):\n","      seed_noise = torch.load(seed_noise_path, map_location=device)\n","else:\n","      seed_noise = torch.randn([1, 13], device=device)\n","      torch.save(seed_noise, seed_noise_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NCptnkWsqa62"},"outputs":[],"source":["'''   imports   '''\n","import pandas as pd\n","from tqdm import tqdm\n","import k_diffusion.models.auto_1\n","import k_diffusion as K\n","import k_diffusion.config as con\n","\n","\n","config_file_name = 'configs/config_ano.json'\n","config = K.config.load_config(open(config_file_name))\n","\n","model_config = config['model']\n","dataset_config = config['dataset']\n","opt_config = config['optimizer']\n","sched_config = config['lr_sched']\n","ema_sched_config = config['ema_sched'] # ema = esponentially moving average\n","\n","\n","opt_config['lr'] = 0.002\n","opt_config['weight_decay'] = 0.00005\n","\n","# autoencoder checkpoint path\n","path = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/Checkpoint/checkpoint.pth'\n","\n","# initialize model\n","gvad_model = K.models.auto_1.GVADModel(\n","    13,\n",")\n","\n","\n","\n","#model, opt, train_dl = accelerator.prepare(gvad_model, opt, loader)\n","inner_model = gvad_model.ae\n","\n","model_denoiser = K.config.make_denoiser_wrapper(config)(inner_model)\n","model_ema = deepcopy(model_denoiser)\n","\n","\n","\n","# load model state to autoencoder\n","checkpoint = torch.load(path)\n","gvad_model.load_state_dict(checkpoint['model_state_dict'])\n","epoch = checkpoint['epoch']\n","\n","#model = con.make_denoiser_wrapper(config)(model)\n","model_ema.load_state_dict(checkpoint['model_ema'])\n","\n","opt = optim.AdamW(gvad_model.parameters(),\n","                  lr=opt_config['lr'],\n","                  betas=tuple(opt_config['betas']),\n","                  eps=opt_config['eps'],\n","                  weight_decay=opt_config['weight_decay'])\n","\n","# scheduler\n","sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,\n","                                                  mode='min',\n","                                                  factor=0.1,\n","                                                  patience=8,\n","                                                  cooldown=0,\n","                                                  verbose=True)\n","\n","\n","\n","\n","ema_sched = K.utils.EMAWarmup(power=ema_sched_config['power'],\n","                              max_value=ema_sched_config['max_value'])\n","\n","\n","opt.load_state_dict(checkpoint['optimizer_state_dict'])\n","sched.load_state_dict(checkpoint['sched'])\n","ema_sched.load_state_dict(checkpoint['ema_sched'])\n","\n","\n","@eval_mode(model_ema)\n","@torch.no_grad()\n","def evaluate(model, sigma_min, sigma_max, loader, n_start, model_ema, dataset):\n","\n","    tqdm.write('Evaluating...')\n","\n","    sigmas = K.sampling.get_sigmas_karras(10, sigma_min, sigma_max, rho=7., device=device)\n","    sigmas = sigmas[n_start:]\n","\n","    sigmas = sigmas.cuda()\n","\n","    # noise is extracted randomly\n","    #sample_noise = torch.randn([1, 13], device=device) * sigma_max\n","\n","    # noise is extracted by file\n","    sample_noise = seed_noise * sigmas[0]\n","\n","    sample_noise = sample_noise.cuda()\n","\n","\n","    def sample_fn(x_real, sigmas, model, model_ema):\n","\n","        x_real = x_real.cuda()\n","        x_real = torch.transpose(x_real, 2,1)\n","\n","        x = sample_noise + x_real\n","\n","        x = x.cuda()\n","        sigmas = sigmas.cuda()\n","        model = model.cuda()\n","        model_ema = model_ema.cuda()\n","\n","        x_0, hidden = K.sampling.sample_lms(model_ema, x, sigmas, disable=True)\n","\n","\n","        return x_0, hidden\n","\n","    def compute_eval_outs_aot(sample_fn, dl,sigmas,model):\n","        outputs = []\n","        test_loss = []\n","\n","\n","\n","        label_list = []\n","        hidden_repr = []\n","\n","        for batch in tqdm(dl):\n","\n","\n","            feat = batch[0]\n","            labels = batch[1]\n","            #print(\"labels\", labels)\n","\n","            #print(\"feat\", feat)\n","\n","\n","            x_0, hidden = sample_fn(feat,sigmas,model, model_ema)\n","\n","\n","            label_list = labels.tolist()\n","\n","            hidden_repr = hidden[0].tolist()\n","\n","\n","            feat = torch.transpose(feat, 2,1)\n","            feat = feat.cuda()\n","            x_0 = x_0.cuda()\n","\n","            #print(\"x_0\", x_0.mean().item())\n","\n","\n","\n","            loss =  model.loss(feat, x_0).mean().item()\n","            #print(\"Loss batch\", loss)\n","\n","            test_loss.append(loss)\n","            outputs.append(x_0)\n","\n","            #print(\"x_0\", x_0)\n","\n","\n","\n","            # save trajectory to csv file\n","\n","            key = str(batch[1])\n","\n","            #print(\"KEY\", key)\n","\n","\n","            '''\n","            x_0 = x_0.cpu()\n","            x_0 = x_0.detach().numpy()\n","            x = x_0[0][0]\n","            y = x_0[0][1]\n","            z = x_0[0][2]\n","\n","\n","            df = pd.DataFrame(list(zip(x, y, z)), columns=['x','y','ratio_wh'])\n","\n","            file_path = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/Dataset/Plot_SameButT/'+ 'MR'  + str(n_start) + '.csv'\n","            df.to_csv(file_path, index=False)\n","            '''\n","\n","\n","\n","\n","        #print(\"output len\", len(outputs))\n","\n","        g_dists = torch.cat(outputs)\n","\n","        #print(\"g_dists\", g_dists.size())\n","\n","        #print(\"OUTPUT\")\n","        #print(outputs[-1].size())\n","\n","        #print(outputs)\n","\n","        test_loss =  sum(test_loss)/len(test_loss)\n","        print(\"loss total: \", test_loss)\n","\n","        return g_dists, label_list, hidden_repr\n","\n","\n","    gen_preds, labels, hiddens = compute_eval_outs_aot(sample_fn, loader,sigmas,model)\n","\n","\n","    '''\n","    label_names = []\n","\n","\n","    for label in labels:\n","      #print(\"label,\", label)\n","      label_name = str(dataset.int_to_label(int(label)))\n","      label_names.append(label_name)\n","\n","\n","    hidden_repr = []\n","    for hidden in hiddens:\n","      #hidden = hidden.cpu()\n","      #hidden = hidden.detach().numpy()\n","      #print(\"hidden\", type(hidden))\n","      a = str(hidden).replace(\"[\", \"\")\n","      a = a.replace(\"]\",\"\")\n","      a = a.replace(\"  \", \" \")\n","      a = a.strip()\n","      hidden_repr.append(a)\n","\n","\n","\n","    df = pd.DataFrame(list(zip(label_names,hidden_repr)), columns=['label','features'])\n","\n","    print(df)\n","\n","    file_path = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/Dataset/T_'+ str(n_start) + \"/\" + 'test' + '.csv'\n","    df.to_csv(file_path, index=False)\n","\n","    #print(\"gen_preds\", gen_preds)\n","    #print(\"gen_preds\", gen_preds.size())\n","    '''\n","\n","dataset_path = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/dataset/SLB/'\n","dataset = CustomDataset(dataset_path)\n","print(len(dataset))\n","\n","\n","loader= DataLoader(dataset, batch_size=len(dataset), shuffle=False, num_workers=0, pin_memory=True)\n","\n","\n","ssd_mean = -4\n","ssd_std = 0.5\n","sigma_min, sigma_max = find_sigma(ssd_mean, ssd_std)\n","for i in range(10):\n","  print(\" Evaluating with t = \", i)\n","  evaluate(model=gvad_model, sigma_min=sigma_min, sigma_max=sigma_max, loader=loader, n_start=i, model_ema = model_ema, dataset = dataset)"]},{"cell_type":"markdown","metadata":{"id":"JkGlGt6qSaEj"},"source":["# MLP\n"]},{"cell_type":"markdown","metadata":{"id":"HwBW7M7x48K_"},"source":["## Datasets and Utils"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"ZCkhnEaBD2lM","executionInfo":{"status":"ok","timestamp":1705319624433,"user_tz":-60,"elapsed":312,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"}}},"outputs":[],"source":["'''   Dataset   '''\n","\n","'''     imports     '''\n","import os\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd\n","import torch\n","\n","'''   custom dataset with learned features    '''\n","class CustomDatasetFeatures(Dataset):\n","    def __init__(self, dataset_path):\n","\n","        self.data = []\n","        self.learned_feats = []\n","        self.labels = []\n","\n","        self.labels_codec = LabelEncoder()\n","\n","        self._init_dataset(dataset_path)\n","\n","    def __len__(self):\n","        return len(self.learned_feats)\n","\n","\n","    def __getitem__(self, idx):\n","        class_id, learn_feat = self.data[idx]\n","\n","        learn_feat = torch.Tensor(learn_feat)\n","\n","        # return both trajectory and class id\n","        return learn_feat, class_id\n","\n","    def _init_dataset(self, dataset_path):\n","\n","        list_behaviors = ['C', 'EP', 'FD', 'FM', 'GC', 'JS', 'MA', 'MR', 'NB','NFM', 'S', 'SSP']\n","        self.labels_codec.fit(list_behaviors)\n","\n","        df = pd.read_csv(dataset_path)\n","        self.labels = [self.label_to_int(i) for i in df['label'].tolist()]\n","\n","        self.learned_feats = df['features'].tolist()\n","\n","        for label, feat in zip(self.labels, self.learned_feats):\n","\n","          feat = list(feat.split(\" \"))\n","          code = []\n","\n","          for l in feat:\n","            if l != '':\n","                if '\\n' in l:\n","                  l = l.replace('\\n','')\n","                if ',' in l:\n","                  l = l.replace(',','')\n","\n","                code.append(float(l))\n","\n","          data = [label, code]\n","          self.data.append(data)\n","\n","\n","    def label_to_int(self, label):\n","        value = self.labels_codec.transform([label])\n","        return value[0]\n","\n","    def int_to_label(self, value):\n","        label = self.labels_codec.inverse_transform([value])\n","        return label[0]"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"QARzw5zQvJUo","executionInfo":{"status":"ok","timestamp":1705319445188,"user_tz":-60,"elapsed":4,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"}}},"outputs":[],"source":["'''   utils   '''\n","\n","class Metric(object):\n","    def __init__(self, name):\n","        self.name = name\n","        self.sum = torch.tensor(0.)\n","        self.n = torch.tensor(0.)\n","\n","    def update(self, val):\n","        self.sum += val.detach().cpu()\n","        self.n += 1\n","\n","    @property\n","    def avg(self):\n","        return self.sum / self.n\n","\n","def random_seed(seed):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","def log(s, nl=True):\n","    print(s, end='\\n' if nl else '')"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"8WUey73ZB4E9","executionInfo":{"status":"ok","timestamp":1705319445188,"user_tz":-60,"elapsed":3,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"}}},"outputs":[],"source":["'''   Focal Loss with fixed alpha    '''\n","'''https://discuss.pytorch.org/t/focal-loss-for-imbalanced-multi-class-classification-in-pytorch/61289/9'''\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, alpha=None, gamma=2):\n","        super(FocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, outputs, targets):\n","        min = 1e-8\n","        max = 1 - min\n","        outputs = torch.clamp(outputs, min, max)\n","\n","        ce_loss = torch.nn.functional.cross_entropy(outputs, targets, reduction='none') # important to add reduction='none' to keep per-batch-item loss\n","        ce_loss = ce_loss.cpu()\n","        pt = torch.exp(-ce_loss)\n","        targets = targets.cpu()\n","        focal_loss = (self.alpha * (1-pt)**self.gamma * ce_loss).mean() # mean over the batch\n","\n","        return focal_loss"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"rIV4HvIrB2eb","executionInfo":{"status":"ok","timestamp":1705319445188,"user_tz":-60,"elapsed":3,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"}}},"outputs":[],"source":["'''   Focal Loss with alpha equal to inverse of class frequencies   '''\n","'''https://saturncloud.io/blog/how-to-use-class-weights-with-focal-loss-in-pytorch-for-imbalanced-multiclass-classification/#:~:text=Focal%20loss%20works%20by%20down,performance%20on%20the%20minority%20class.'''\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, alpha=None, gamma=2):\n","        super(FocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, inputs, targets):\n","        min = 1e-8\n","        max = 1 - min\n","        inputs = torch.clamp(inputs, min, max)\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n","\n","        pt = torch.exp(-ce_loss)\n","        targets = targets.cpu()\n","        pt = pt.cpu()\n","        ce_loss = ce_loss.cpu()\n","        loss = (self.alpha[targets] * (1 - pt) ** self.gamma * ce_loss).mean()\n","        return loss"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"s0UC7yhGSbph","executionInfo":{"status":"ok","timestamp":1705319445188,"user_tz":-60,"elapsed":3,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"}}},"outputs":[],"source":["'''   MLP model   '''\n","\n","import torch.nn as nn\n","\n","class MLP(nn.Module):\n","    def __init__(\n","            self,\n","            encoder_latent_space,\n","            output_classes\n","    ):\n","        super().__init__()\n","        self.mlp = nn.Sequential(\n","            nn.Linear(encoder_latent_space, 256),\n","            nn.PReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(256, 128),\n","            nn.PReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(128, 64),\n","            nn.PReLU(),\n","            nn.Dropout(0.1),\n","            nn.BatchNorm1d(64),\n","            nn.Linear(64, output_classes),\n","        )\n","\n","    def forward(self, x):\n","        return self.mlp(x)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"NyeFiBCTQVgS","executionInfo":{"status":"ok","timestamp":1705319445189,"user_tz":-60,"elapsed":3,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"}}},"outputs":[],"source":["'''   Dataset Smote  '''\n","\n","'''     imports     '''\n","import os\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd\n","import torch\n","\n","'''   custom dataset with learned features    '''\n","class CustomDatasetSmote(Dataset):\n","    def __init__(self, x, y):\n","\n","        self.data = []\n","        self.learn_feats = []\n","        self.labels = []\n","\n","        self.labels_codec = LabelEncoder()\n","\n","        self._init_dataset(x, y)\n","\n","    def __len__(self):\n","        return len(self.learn_feats)\n","\n","\n","    def __getitem__(self, idx):\n","        class_id, learn_feat = self.data[idx]\n","\n","        learn_feat = torch.Tensor(learn_feat)\n","\n","        # return both trajectory and class id\n","        return learn_feat, class_id\n","\n","    def _init_dataset(self, x, y):\n","\n","        list_behaviors = ['C', 'EP', 'FD', 'FM', 'GC', 'JS', 'MA', 'MR', 'NB','NFM', 'S', 'SSP']\n","        self.labels_codec.fit(list_behaviors)\n","\n","        for label, feat in zip(y,x):\n","            data = [label, feat]\n","            self.data.append(data)\n","\n","        self.labels = y\n","        self.learn_feats = x\n","\n","\n","    def label_to_int(self, label):\n","        value = self.labels_codec.transform([label])\n","        return value[0]\n","\n","    def int_to_label(self, value):\n","        label = self.labels_codec.inverse_transform([value])\n","        return label[0]"]},{"cell_type":"markdown","metadata":{"id":"1Sy5REo6_fuL"},"source":["## Train"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"2VPOZR25S5Ez","executionInfo":{"status":"ok","timestamp":1705319638343,"user_tz":-60,"elapsed":439,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"}}},"outputs":[],"source":["'''  imports   '''\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","from imblearn.metrics import geometric_mean_score\n","\n","\n","'''   training    '''\n","def train(model, optimizer, scheduler, criterion, epochs, device, train_loader, test_loader, save_weights_name, batch_size, save_model=False, wandb_update=False):\n","    model.to(device)\n","\n","\n","    for e in range(epochs):\n","\n","        '''       training    '''\n","        print('training')\n","        model.train()\n","\n","        trainF = Metric('trainF')\n","\n","        progress_bar = tqdm(train_loader, desc='description')\n","\n","        overall_preds = []\n","        overall_targets = []\n","        softmax = torch.nn.Softmax(dim=1)\n","\n","\n","        for code, targets in progress_bar:\n","\n","            code = code.to(device)\n","            targets = targets.to(device)\n","\n","            # forward\n","            outputs = model(code)\n","\n","            # zero grad\n","            optimizer.zero_grad()\n","\n","            # compute loss\n","            loss = criterion(outputs, targets)\n","\n","            loss.div_(math.ceil(float(len(code)) / batch_size))\n","\n","            # softmax tha outputs and take the max index (argmax), that is the final predicted class\n","            preds = torch.argmax(softmax(outputs), dim=1).cpu()\n","\n","            # save results and targets for accuracy metrics\n","            overall_preds += preds.tolist()\n","            overall_targets += targets.cpu().tolist()\n","\n","            # backpropagate the error\n","            loss.backward()\n","\n","            # clip gradient\n","            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # update weights\n","            optimizer.step()\n","\n","            # update loss\n","            trainF.update(loss)\n","\n","\n","        model.eval()\n","        train_accuracy = accuracy_score(overall_targets, overall_preds)\n","        train_f1 = f1_score(overall_targets, overall_preds, average='macro')\n","\n","\n","        # save loss for epoch\n","        train_log = {'Train F': trainF.avg.item(),\n","                     'Train A': train_accuracy,\n","                     'Train F1': train_f1}\n","\n","        log('--------------------')\n","        log('Epoch: {}'.format(e))\n","        log('Train Focal Loss:{}'.format(train_log['Train F']))\n","        log('Train Accuracy:{}'.format(train_accuracy))\n","        log('Train F1 score:{}'.format(train_f1))\n","\n","\n","        '''       test      '''\n","        print(\"test\")\n","        model.eval()\n","        testF = Metric('testF')\n","\n","        with torch.no_grad():\n","          progress_bar = tqdm(test_loader, desc='description')\n","          overall_preds = []\n","          overall_targets = []\n","          softmax = torch.nn.Softmax(dim=1)\n","\n","          for code, targets in progress_bar:\n","              code = code.to(device)\n","              targets = targets.to(device)\n","\n","              # forward\n","              outputs = model(code)\n","\n","              # compute testing loss\n","              loss = criterion(outputs, targets)\n","\n","              # softmax tha outputs and take the max index (argmax), that is the final predicted class\n","              preds = torch.argmax(softmax(outputs), dim=1).cpu()\n","\n","              # save results and targets for accuracy metrics\n","              overall_preds += preds.tolist()\n","              overall_targets += targets.cpu().tolist()\n","\n","              testF.update(loss)\n","\n","          test_accuracy = accuracy_score(overall_targets, overall_preds)\n","          test_f1 = f1_score(overall_targets, overall_preds, average='macro')\n","\n","          geo_w = geometric_mean_score(overall_targets, overall_preds, average='weighted')\n","          geo_m = geometric_mean_score(overall_targets, overall_preds, average='macro')\n","\n","          test_log = {'Test F': testF.avg.item(),\n","                       'Test A': test_accuracy,\n","                      'Test F1': test_f1,\n","                      'Test Geo W': geo_w,\n","                      'Test Geo M': geo_m}\n","\n","          log('Test Focal Loss:{}'.format(test_log['Test F']))\n","          log('Test Accuracy:{}'.format(test_log['Test A']))\n","          log('Test F1:{}'.format(test_log['Test F1']))\n","          log('Test Geo Weighted:{}'.format(test_log['Test Geo W']))\n","          log('Test Geo Macro:{}'.format(test_log['Test Geo M']))\n","\n","        '''\n","        # confusion matrix\n","        if test_f1 >= 0.5:\n","          classes = ['C', 'EP', 'FD', 'FM', 'GC', 'JS', 'MA', 'MR', 'NB','NFM', 'S', 'SSP']\n","\n","\n","          cf_matrix = confusion_matrix(overall_targets, overall_preds)\n","          print(\"confusion matrix\")\n","          df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n","                              columns = [i for i in classes])\n","          print(\"data frame\")\n","          plt.figure(figsize = (12,7))\n","          sn.heatmap(df_cm, annot=True)\n","          #plt.show()\n","          #plt.savefig('/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/ConfusionMatrix' + str(test_f1) +  '.png')\n","\n","        '''\n","        '''\n","        # save model\n","        if test_f1 >= 0.5:\n","            path = \"/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/Checkpoint/MLP/\"\n","            #name = \"MLP\" + str(100. * train_log['Train A'])\n","            name = \"MLPSmote\" + str(100. * test_f1)\n","            torch.save({\n","                'epoch': e+1,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict()\n","            },f'{path}/{name}.pth')\n","        '''\n","\n","        # update wandb\n","        if wandb_update:\n","          wandb.log({'Epochs': e + 1,\n","                    'Train Focal Loss': train_log['Train F'],\n","                     'Train Accuracy': train_log['Train A'],\n","                     'Train F1': train_log['Train F1'],\n","                     'Test Focal Loss': test_log['Test F'],\n","                     'Test Accuracy': test_log['Test A'],\n","                     'Test F1': test_log['Test F1'],\n","                     'Test Geo Weighted': test_log['Test Geo W'],\n","                     'Test Geo Macro':test_log['Test Geo M']})\n","\n","        scheduler.step(train_log['Train F'])"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"mWST9jdlSvGq","executionInfo":{"status":"ok","timestamp":1705319708206,"user_tz":-60,"elapsed":3,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"}}},"outputs":[],"source":["'''   imports  '''\n","from tqdm import tqdm\n","import imblearn\n","from array import array\n","import numpy as np\n","from collections import Counter\n","from imblearn.over_sampling import SMOTE\n","\n","'''   main    '''\n","def main():\n","\n","        focal = False\n","        smote = True\n","        #t = 5\n","        lr = 5e-5\n","        batch_size = 16\n","        n_workers = 2\n","        epochs = 200\n","        wandb_update = False\n","        weight_decay = 0.001\n","        #weight_decay = 0\n","        gamma = 3\n","        hidden_space_dim = 512\n","\n","\n","\n","        for t in range(10):\n","\n","          seed = 3\n","\n","          # initialize random seeds\n","          random_seed(seed)\n","\n","          mlp = MLP(\n","              encoder_latent_space = hidden_space_dim,\n","              output_classes = 12\n","          )\n","\n","          df_weight = pd.read_csv('/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/dataset/train_class_weight.csv')\n","          class_weights = df_weight['weight'].tolist()\n","          class_weights = torch.FloatTensor(class_weights)\n","\n","          if focal:\n","            criterion = FocalLoss(alpha=class_weights, gamma=gamma)\n","\n","          else:\n","            criterion = nn.CrossEntropyLoss()\n","\n","          optimizer = optim.Adam(mlp.parameters(), lr = lr, weight_decay=weight_decay)\n","          scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n","                                                                  mode='min',\n","                                                                  factor=0.1,\n","                                                                  patience=10,\n","                                                                  cooldown=0,\n","                                                                  verbose=True)\n","\n","          device = \"cuda:0\"\n","\n","\n","          if wandb_update:\n","            # save model inputs and hyperparameters\n","            config = {\n","              \"t\": t,\n","              \"lr\": lr,\n","              \"batch_size\": batch_size,\n","              \"epochs\": epochs,\n","              \"features_size\": hidden_space_dim,\n","              \"loss focal\": focal,\n","              \"smote\": smote,\n","              \"gamma\": gamma,\n","              \"weight_decay\": weight_decay,\n","              \"dropout\": \"dropout\"\n","            }\n","\n","            # start a new wandb run\n","            run = wandb.init(project='project_name', entity='entity_name', config=config, settings=wandb.Settings(start_method=\"thread\"))\n","\n","            # log gradients and model parameters\n","            wandb.watch(mlp)\n","\n","\n","          # load datasets\n","          # train dataset\n","          path_train = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/Dataset/T_' + str(t) + '/train.csv'\n","          dataset_train = CustomDatasetFeatures(path_train)\n","          dataloader_train = DataLoader(dataset_train, shuffle = True, batch_size=batch_size, drop_last=True)\n","          print(len(dataset_train))\n","\n","          # test dataset\n","          path_test = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/Dataset/T_' + str(t) + '/test.csv'\n","          dataset_test = CustomDatasetFeatures(path_test)\n","          dataloader_test = DataLoader(dataset_test, shuffle = True, batch_size=batch_size, num_workers=n_workers, pin_memory=True)\n","          print(len(dataset_test))\n","\n","          # smote\n","          if smote:\n","\n","            to_sample = {1:33, 3: 114, 4: 18, 7: 9}\n","\n","            oversample = SMOTE(k_neighbors=2, sampling_strategy=to_sample)\n","\n","            x = []\n","            y = []\n","            for i in range(len(dataset_train)):\n","              array = dataset_train[i][0].numpy()\n","              x.append(array)\n","              y.append(dataset_train[i][1])\n","\n","            from collections import Counter\n","            y_counter = Counter(y)\n","            print(y_counter)\n","\n","            total_samples = len(y)\n","\n","\n","            res = np.array(x)\n","            res = res.reshape(13489, -1)\n","            res, y = oversample.fit_resample(res,y)\n","\n","            from collections import Counter\n","            print(Counter(y))\n","\n","            dataset = CustomDatasetSmote(res, y)\n","            dataloader_train = DataLoader(dataset, shuffle = True, batch_size=batch_size)\n","            print(len(dataset))\n","\n","\n","\n","          # train\n","          train(\n","              model = mlp,\n","              optimizer = optimizer,\n","              scheduler = scheduler,\n","              criterion = criterion,\n","              epochs = epochs,\n","              device = device,\n","              train_loader = dataloader_train,\n","              test_loader = dataloader_test,\n","              save_weights_name = 'mlp.pth',\n","              batch_size = batch_size,\n","              wandb_update = wandb_update\n","          )\n","\n","          if wandb_update:\n","            run.finish()"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"m3GwmNIzYh20","executionInfo":{"status":"ok","timestamp":1705319485589,"user_tz":-60,"elapsed":367,"user":{"displayName":"Noemi Canovi","userId":"00534622395900487090"}}},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"stewnNkNXETl"},"outputs":[],"source":["!wandb login"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysOvc0ANTS3c"},"outputs":[],"source":["main()"]},{"cell_type":"markdown","metadata":{"id":"7avg1VulsZpn"},"source":["## Evaluation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rpEf0t91KJyi"},"outputs":[],"source":["'''   imports   '''\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","import pandas as pd\n","import matplotlib\n","import matplotlib.colors as mcolors\n","from imblearn.metrics import geometric_mean_score\n","\n","'''   testing   '''\n","def test(model, criterion, device, data_loader, dataset, load_weights = False, file_name = None, wandb_update=False):\n","\n","    if load_weights:\n","        checkpoint = torch.load(file_name)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        print('Weights loaded.')\n","\n","\n","    model.eval()\n","\n","    testCE = Metric('testCE')\n","\n","    with torch.no_grad():\n","        progress_bar = tqdm(data_loader, desc='description')\n","\n","        overall_preds = []\n","        overall_targets = []\n","        softmax = torch.nn.Softmax(dim=1)\n","        for code, targets in progress_bar:\n","            code = code.to(device)\n","            targets = targets.to(device)\n","\n","            # forward\n","            outputs = model(code)\n","\n","            # compute testing loss\n","            loss = criterion(outputs, targets)\n","\n","            soft = softmax(outputs).cpu()\n","\n","            # softmax tha outputs and take the max index (argmax), that is the final predicted class\n","            preds = torch.argmax(softmax(outputs), dim=1).cpu()\n","\n","            # save results and targets for accuracy metrics\n","            overall_preds += preds.tolist()\n","            overall_targets += targets.cpu().tolist()\n","\n","            testCE.update(loss)\n","\n","        test_accuracy = accuracy_score(overall_targets, overall_preds)\n","        f1_w = f1_score(overall_targets, overall_preds, average='weighted')\n","        f1_m = f1_score(overall_targets, overall_preds, average='micro')\n","        f1_M = f1_score(overall_targets, overall_preds, average='macro')\n","\n","        print(\"predictions:\")\n","        print(overall_preds)\n","        print(\"labels\")\n","        print(overall_targets)\n","\n","        test_log = {'Test CE': testCE.avg.item(),\n","                      'Test A': test_accuracy}\n","\n","\n","        geo_weighted = geometric_mean_score(overall_targets, overall_preds, average='weighted')\n","        geo_micro = geometric_mean_score(overall_targets, overall_preds, average='micro')\n","        geo_macro = geometric_mean_score(overall_targets, overall_preds, average='macro')\n","\n","        print(\"accuracy: \", test_accuracy)\n","        print(\"f1 score: \", f1_w)\n","        print(\"f1 score macro: \", f1_M)\n","        print(\"geo_weighted: \", geo_weighted)\n","        print(\"geo_micro: \", geo_micro)\n","        print(\"geo_macro: \", geo_macro)\n","\n","\n","        log('Test CE:{}'.format(test_log['Test CE']))\n","        log('Test Accuracy:{}'.format(test_log['Test A']))\n","\n","\n","        classes = ['C', 'EP', 'FD', 'FM', 'GC', 'JS', 'MA', 'MR', 'NB','NFM', 'S', 'SSP']\n","\n","\n","        # confusion matrix\n","\n","        cf_matrix = confusion_matrix(overall_targets, overall_preds)\n","        print(\"confusion matrix\")\n","        df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n","                            columns = [i for i in classes])\n","        print(\"data frame\")\n","        plt.figure(figsize = (12,7))\n","        sn.heatmap(df_cm, annot=True)\n","\n","        #plt.savefig('/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/ConfusionMatrixBestF1Macro.png')\n","\n","        plt.show()\n","\n","\n","        preds = [dataset.int_to_label(i) for i in overall_preds]\n","        labels = [dataset.int_to_label(i) for i in overall_targets]\n","\n","\n","        # accuracy for class\n","\n","        acc = []\n","        n_test = []\n","        for c in classes:\n","          correct = 0\n","          tot = 0\n","\n","          for i in range(len(labels)):\n","              if labels[i] == c:\n","                  if preds[i] == c:\n","                      correct += 1\n","                  tot += 1\n","          try:\n","              acc_class = correct/tot\n","          except:\n","              print(\"class has no sample\")\n","              acc_class = \"Nan\"\n","\n","          acc.append(acc_class)\n","          n_test.append(tot)\n","\n","\n","          #print(\"type\", acc_class, type(acc_class))\n","\n","\n","        #print(\"accuracy classes\", acc)\n","        n_tot = [174, 13, 14457, 48, 8, 1019, 67, 4, 198, 895, 31, 23]\n","\n","        df = pd.DataFrame(list(zip( acc, n_tot, n_test,)), index=[i for i in classes], columns = [ 'Class accuracy', '# samples (tot)', '# samples (test)' ])\n","\n","        plt.figure(figsize = (12,7))\n","\n","        norm = mcolors.Normalize(-1,1)\n","\n","        colors = [[norm(-1.0), \"whitesmoke\"],\n","            [norm( 1.0), \"whitesmoke\"]]\n","\n","        cmap = mcolors.LinearSegmentedColormap.from_list(\"\", colors)\n","        hm = sn.heatmap(df, annot=True, cmap=cmap, cbar=False, linecolor=\"black\",fmt='g')\n","\n","        plt.yticks(rotation='horizontal')\n","        plt.xticks()\n","\n","        plt.savefig('/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/Table.png')\n","        plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iEwirbnjKO8i"},"outputs":[],"source":["def eval():\n","\n","    t = 5\n","    lr = 5e-5\n","    batch_size = 16\n","    n_workers = 2\n","    wandb_update = False\n","    weight_decay = 1e-4\n","\n","    smaller_space_dim = 512\n","\n","    seed = 3\n","\n","    # initialize random seeds\n","    random_seed(seed)\n","\n","    mlp = MLP(\n","        encoder_latent_space = smaller_space_dim,\n","        output_classes = 12\n","    )\n","\n","    # extract class weights from csv file\n","    df_weight = pd.read_csv('/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Autoencoder/dataset/train_class_weight.csv')\n","    class_weights = df_weight['weight'].tolist()\n","    class_weights = torch.FloatTensor(class_weights)\n","\n","    #criterion = FocalLoss(alpha=class_weights, gamma=2)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(mlp.parameters(), lr = lr, weight_decay=weight_decay)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n","                                                           mode='min',\n","                                                           factor=0.1,\n","                                                           patience=8,\n","                                                           cooldown=0,\n","                                                           verbose=True)\n","\n","    device = \"cuda:0\"\n","    mlp.to(device)\n","\n","    # load datasets\n","    # train dataset\n","    path_train = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/Dataset/T_'+ str(t) + '/train.csv'\n","    dataset_train = CustomDatasetFeatures(path_train)\n","    dataloader_train = DataLoader(dataset_train, shuffle = True, batch_size=batch_size)\n","    print(len(dataset_train))\n","\n","    # test dataset\n","    path_test = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/Dataset/T_'+ str(t) + '/test.csv'\n","    dataset_test = CustomDatasetFeatures(path_test)\n","    dataloader_test = DataLoader(dataset_test, shuffle = True, batch_size=batch_size, num_workers=n_workers, pin_memory=True)\n","    print(len(dataset_test))\n","\n","    #def test(model, criterion, device, data_loader, load_weights = False, file_name = None, wandb_update=False):\n","    # train\n","    test(\n","        model = mlp,\n","        criterion = criterion,\n","        device = device,\n","        data_loader = dataloader_test,\n","        dataset = dataset_test,\n","        file_name = '/content/drive/MyDrive/Behavior_CorkwingWrasse_CoastVision/Diffusion/Checkpoint/MLP/MLPF47.80931373868435.pth',\n","        load_weights = True,\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UJorvlBDKPZI"},"outputs":[],"source":["eval()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["hJgOl87-vBgT","mIsQ1F_4vSsD","7avg1VulsZpn"],"toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}